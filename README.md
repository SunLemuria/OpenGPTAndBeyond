# open source ChatGPT and beyond

On the road to implement open-source ChatGPT-like models and beyond.

Since the accidental leak of LLaMA model weights, and the impressive performance of Stanford Alpaca, which is trained on LLaMA using data generated by GPT-3 api with the self-instruct technique, the open-source community has been excited about the promising future of reproducing ChatGPT in an open way.

This repo aims at recording this process, and providing an overview of how to get involved.

Including: base models, technologies, data, domain models, training pipelines, speed up techniques, multi-language, multi-modal, and more to go.

thanks [@FunnySaltyFish](https://github.com/FunnySaltyFish) for the [website version](https://llm.best/), here is the [code](https://github.com/FunnySaltyFish/best_llm).

Any contribution to this project and the website is appreciated! (we are short of hands ...)

# Base Models

| contributor                                | model/project                                                                                   | multi-modal | license    | language | main feature                                                                                                                                                                                                                                                                                                                                                                    |
| ------------------------------------------ | ----------------------------------------------------------------------------------------------- | ----------- | ---------- | -------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Meta                                       | [LLaMA](https://github.com/facebookresearch/llama)                                                 | &#x2716;    |            | en       | LLaMA-13B outperforms GPT-3(175B) and LLaMA-65B is competitive to PaLM-540M.<br />Base model for most follow-up works.                                                                                                                                                                                                                                                          |
| THU                                        | [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)                                                  | &#x2716;    |            | en/zh    | well-known Chinese model, in chat mode, and can run on single GPU.                                                                                                                                                                                                                                                                                                              |
| HuggingFace-BigScience                     | [BLOOM](https://huggingface.co/bigscience/bloom)                                                   | &#x2716;    |            | multi    | an autoregressive Large Language Model (LLM) trained by HuggingFace BigScience.                                                                                                                                                                                                                                                                                                 |
| HuggingFace-BigScience                     | [BLOOMZ](https://huggingface.co/bigscience/bloomz)                                                 | &#x2716;    |            | multi    | instruction-finetuned version of BLOOM & mT5 pretrained multilingual language models on crosslingual task mixture.                                                                                                                                                                                                                                                             |
| EleutherAI                                 | [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6b)                                                | &#x2716;    |            | en       | transformer model trained using Ben Wang's[Mesh Transformer JAX](https://github.com/kingoflolz/mesh-transformer-jax/).                                                                                                                                                                                                                                                             |
| Meta                                       | [OPT](https://huggingface.co/facebook/opt-66b)                                                     | &#x2716;    |            | en       | Open Pre-trained Transformer Language Models, aim in developing this suite of OPT models is to enable reproducible<br /> and responsible research at scale, and to bring more voices to the table in studying the impact of these LLMs.                                                                                                                                        |
| [Cerebras Systems](https://www.cerebras.net/) | [Cerebras-GPT](https://huggingface.co/cerebras/Cerebras-GPT-13B)                                   | &#x2716;    |            | en       | Pretrained LLM, GPT-3 like, Commercially available, efficiently trained on the[Andromeda](https://www.cerebras.net/andromeda/) AI supercomputer,<br />trained in accordance with[Chinchilla scaling laws](https://arxiv.org/abs/2203.15556) (20 tokens per model parameter) which is compute-optimal.                                                                                |
| EleutherAI                                 | [pythia](https://github.com/EleutherAI/pythia)                                                     | &#x2716;    |            | en       | combine interpretability analysis and scaling laws to understand how knowledge develops<br />and evolves during training in autoregressive transformers.                                                                                                                                                                                                                        |
| Stability-AI                               | [StableLM](https://github.com/Stability-AI/StableLM)                                               | &#x2716;    |            | en       | Stability AI Language Models                                                                                                                                                                                                                                                                                                                                                    |
| FDU                                        | [MOSS](https://github.com/OpenLMLab/MOSS)                                                          | &#x2716;    |            | en/zh    | An open-source tool-augmented conversational language model from Fudan University.                                                                                                                                                                                                                                                                                              |
| ssymmetry & FDU                            | [BBT-2](https://bbt.ssymmetry.com/)                                                                | &#x2716;    |            | zh       | 12B open-source LM.                                                                                                                                                                                                                                                                                                                                                             |
| @mlfoundations                             | [OpenFlamingo](https://github.com/mlfoundations/open_flamingo)                                     | &#x2705;    |            | en       | An open-source framework for training large multimodal models.                                                                                                                                                                                                                                                                                                                  |
| EleutherAI                                 | [GPT-NeoX-20B](https://huggingface.co/EleutherAI/gpt-neox-20b)                                     | &#x2716;    |            | en       | Its architecture intentionally resembles that of GPT-3, and is almost identical to that of[GPT-J- 6B](https://huggingface.co/EleutherAI/gpt-j-6B).                                                                                                                                                                                                                                 |
| UCB                                        | [OpenLLaMA](https://github.com/openlm-research/open_llama)                                         | &#x2716;    | Apache-2.0 | en       | An Open Reproduction of LLaMA.                                                                                                                                                                                                                                                                                                                                                  |
| MosaicML                                   | [MPT](https://github.com/mosaicml/llm-foundry)                                                     | &#x2716;    | Apache-2.0 | en       | MPT-7B is a GPT-style model, and the first in the MosaicML Foundation Series of models.<br /> Trained on 1T tokens of a MosaicML-curated dataset, MPT-7B is open-source,<br /> commercially usable, and equivalent to LLaMa 7B on evaluation metrics.                                                                                                                           |
| TogetherComputer                           | [RedPajama-INCITE-Base-3B-v1](https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1) | &#x2716;    | Apache-2.0 | en       | A 2.8B parameter pretrained language model, pretrained on[RedPajama-Data-1T](https://huggingface.co/models?dataset=dataset:togethercomputer/RedPajama-Data-1T),<br /> together with an [Instruction-tuned Version](https://huggingface.co/togethercomputer/RedPajama-INCITE-Instruct-3B-v1) and a [Chat Version](https://huggingface.co/togethercomputer/RedPajama-INCITE-Chat-3B-v1). |
| Lightning-AI                               | [Lit-LLaMA](https://github.com/Lightning-AI/lit-llama)                                             | &#x2716;    | Apache-2.0 | -        | Independent implementation of[LLaMA](https://github.com/facebookresearch/llama) that is fully open source under the **Apache 2.0 license.**                                                                                                                                                                                                                                  |

# Domain Models

| contributor                        | model                                                                                                             | domain          | language | base model   | main feature                                                                                                                                                                                                                           |
| ---------------------------------- | ----------------------------------------------------------------------------------------------------------------- | --------------- | -------- | ------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| UT Southwestern/<br />UIUC/OSU/HDU | [ChatDoctor](https://github.com/Kent0n-Li/ChatDoctor)                                                                | medical         | en       | LLaMA         | Maybe the first domain-specific chat model tuned on LLaMA.                                                                                                                                                                             |
| Cambridge                          | [Visual Med-Alpaca](https://github.com/cambridgeltl/visual-med-alpaca)                                               | biomedical      | en       | LLaMA-7B      | a multi-modal foundation model designed specifically for the biomedical domain.                                                                                                                                                        |
| HIT                                | [Huatuo](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese) / [ChatGLM-Med](https://github.com/SCIR-HI/Med-ChatGLM) | medical         | zh       | LLaMA/ChatGLM | ine-tuned with Chinese medical knowledge dataset, which is generated by using gpt3.5 api.                                                                                                                                              |
| ShanghaiTech, etc                  | [DoctorGLM](https://github.com/xionghonglin/DoctorGLM)                                                               | medical         | en/zh    | ChatGLM-6B    | Chinese medical consultation model fine-tuned on ChatGLM-6B.                                                                                                                                                                          |
| THU AIR                            | [BioMedGPT-1.6B](https://github.com/BioFM/OpenBioMed)                                                                | biomedical      | en/zh    | -             | a pre-trained multi-modal molecular foundation model with 1.6B parameters that associates 2D molecular graphs with texts.                                                                                                              |
| @LiuHC0428                         | [LawGPT_zh](https://github.com/LiuHC0428/LAW-GPT)                                                                    | legal           | zh       | ChatGLM-6B    | a general model in Chinese legal domain, trained on data generated via Reliable-Self-Instruction.                                                                                                                                    |
| SJTU                               | [MedicalGPT-zh](https://github.com/MediaBrain-SJTU/MedicalGPT-zh)                                                    | medical         | zh       | ChatGLM-6B    | a general model in Chinese medical domain, a diverse data generated via self-instruct.                                                                                                                                                |
| SJTU                               | [PMC-LLaMA](https://github.com/chaoyi-wu/PMC-LLaMA)                                                                  | medical         | zh       | LLaMA         | Continue Training LLaMA on Medical Papers.                                                                                                                                                                                             |
| HuggingFace                        | [StarCoder](https://github.com/bigcode-project/starcoder)                                                            | code generation | en       | -             | a language model (LM) trained on source code and natural language text. Its training data incorporates more than<br /> 80 different programming languages as well as text extracted from GitHub issues and commits and from notebooks. |
| @CogStack                          | [NHS-LLM](https://github.com/CogStack/opengpt#nhs-llm)                                                               | medical         | en       | not clear     | A conversational model for healthcare trained using[OpenGPT](https://github.com/CogStack/opengpt).                                                                                                                                        |
| @pengxiao-song                     | [LaWGPT](https://github.com/pengxiao-song/LaWGPT)                                                                    | legal           | zh       | LLaMA/ChatGLM | expand the vocab with Chinese legal terminologies, instruction fine-tuned on data generated using self-instruct.                                                                                                                      |

# General Domain Instruction Models

| contributor                       | model/project                                                                      | language | base model                                | main feature                                                                                                                                                                                                                                                                                                                                                                         |
| :-------------------------------- | :--------------------------------------------------------------------------------- | -------- | :----------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Stanford                          | [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)                                | en       | LLaMA/OPT                                  | use 52K instruction-following data generated by Self-Instructt techniques to fine-tune 7B LLaMA,<br /> the resulting model,  Alpaca, behaves similarly to the `text-davinci-003` model on the Self-Instruct instruction-following evaluation suite.<br />Alpaca has inspired many follow-up models.                                                                           |
| LianJiaTech                       | [BELLE](https://github.com/LianjiaTech/BELLE)                                         | en/zh    | BLOOMZ-7B1-mt                              | maybe the first Chinese model to follow Alpaca.                                                                                                                                                                                                                                                                                                                                      |
| Databricks                        | [Dolly](https://github.com/databrickslabs/dolly)                                      | en       | GPT-J 6B                                   | use Alpaca data to fine-tune a 2-year-old model: GPT-J, which exhibits surprisingly high quality<br /> instruction following behavior not characteristic of the foundation model on which it is based.                                                                                                                                                                              |
| @tloen                            | [Alpaca-LoRA](https://github.com/tloen/alpaca-lora)                                   | en       | LLaMA-7B                                   | trained within hours on a single RTX 4090,<br />reproducing the [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) results using [low-rank adaptation (LoRA)](https://arxiv.org/pdf/2106.09685.pdf),<br />and can run on a Raspberry pi.                                                                                                                                     |
| ColossalAI                        | [Coati7B]()                                                                           | en/zh    | LLaMA-7B                                   | a large language model developed by the ColossalChat project                                                                                                                                                                                                                                                                                                                         |
| Shanghai AI Lab                   | [LLaMA-Adapter](https://github.com/ZrrSkywalker/LLaMA-Adapter)                        | en       | LLaMA-7B                                   | Fine-tuning LLaMA to follow instructions within 1 Hour and 1.2M Parameters                                                                                                                                                                                                                                                                                                           |
| AetherCortex                      | [Llama-X](https://github.com/AetherCortex/Llama-X)                                    | en       | LLaMA                                      | Open Academic Research on Improving LLaMA to SOTA LLM.                                                                                                                                                                                                                                                                                                                               |
| TogetherComputer                  | [OpenChatKit](https://github.com/togethercomputer/OpenChatKit)                        | en       | GPT-NeoX-20B                               | OpenChatKit provides a powerful, open-source base to create both specialized and general purpose chatbots for various applications.<br /> The kit includes an instruction-tuned language models, a moderation model, and an extensible retrieval system for including <br />up-to-date responses from custom repositories.                                                           |
| nomic-ai                          | [GPT4All](https://github.com/nomic-ai/gpt4all)                                        | en       | LLaMA                                      | trained on a massive collection of clean assistant data including code, stories and dialogue                                                                                                                                                                                                                                                                                         |
| @ymcui                            | [Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)                 | en/zh    | LLaMA-7B/13B                               | **expand the Chinese vocabulary** based on the original LLaMA and use Chinese data for secondary pre-training,<br /> further enhancing Chinese basic semantic understanding. Additionally, the project uses Chinese instruction data<br /> for fine-tuning on the basis of the Chinese LLaMA, significantly improving the model's understanding and execution of instructions. |
| UC Berkley<br />Stanford<br />CMU | [Vicuna](https://github.com/lm-sys/FastChat)                                          | en       | LLaMA-13B                                  | Impressing GPT-4 with 90% ChatGPT Quality.                                                                                                                                                                                                                                                                                                                                           |
| UCSD/SYSU                         | [baize](https://github.com/project-baize/baize)                                       | en/zh    | LLaMA                                      | fine-tuned with[LoRA](https://github.com/microsoft/LoRA). It uses 100k dialogs generated by letting ChatGPT chat with itself. <br />Alpaca's data is also used to improve its performance.                                                                                                                                                                                              |
| UC Berkley                        | [Koala](https://github.com/young-geng/EasyLM)                                         | en       | LLaMA                                      | Rather than maximizing*quantity* by scraping as much web data as possible, the team focus on collecting a small *high-quality* dataset.                                                                                                                                                                                                                                          |
| @imClumsyPanda                    | [langchain-ChatGLM](https://github.com/imClumsyPanda/langchain-ChatGLM)               | en/zh    | ChatGLM-6B                                 | local knowledge based ChatGLM with langchain.                                                                                                                                                                                                                                                                                                                                        |
| @yangjianxin1                     | [Firefly](https://github.com/yangjianxin1/Firefly)                                    | zh       | bloom-1b4-zh<br />bloom-2b6-zh             | Instruction Tuning on Chinese dataset. Vocabulary pruning, ZeRO, and tensor parallelism<br /> are used to effectively reduce memory consumption and improve training efficiency.                                                                                                                                                                                                     |
| microsoft                         | [GPT-4-LLM](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)               | en/zh    | LLaMA                                      | aims to share data generated by GPT-4 for building an instruction-following LLMs with supervised learning and reinforcement learning.                                                                                                                                                                                                                                                |
| Hugging Face                      | [StackLLaMA](https://huggingface.co/trl-lib/llama-7b-se-rl-peft)                      | en       | LLaMA                                      | trained on StackExchange data and the main goal is to serve as a tutorial and walkthrough on<br /> how to train model with RLHF and not primarily model performance.                                                                                                                                                                                                                |
| Nebuly                            | [ChatLLaMA](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllam) | en       | -                                          | a library that allows you to create hyper-personalized ChatGPT-like assistants using your own data and the least amount of compute possible.                                                                                                                                                                                                                                        |
| @juncongmoo                       | [ChatLLaMA](https://github.com/juncongmoo/chatllama)                                  | en       | LLaMA                                      | LLaMA-based RLHF model, runnable in a single GPU.                                                                                                                                                                                                                                                                                                                                   |
| @juncongmoo                       | [minichatgpt](https://github.com/juncongmoo/minichatgpt)                              | en       | GPT/OPT ...                                | To Train ChatGPT In 5 Minutes with ColossalAI.                                                                                                                                                                                                                                                                                                                                       |
| @LC1332                           | [Luotuo-Chinese-LLM](https://github.com/LC1332/Luotuo-Chinese-LLM)                    | zh       | LLaMA/ChatGLM                              | Instruction fine-tuned Chinese Language Models, with colab provided!                                                                                                                                                                                                                                                                                                                 |
| @Facico                           | [Chinese-Vicuna](https://github.com/Facico/Chinese-Vicuna)                            | zh       | LLaMA                                      | A Chinese Instruction-following LLaMA-based Model, fine-tuned with Lora, cpp inference supported, colab provided.                                                                                                                                                                                                                                                                    |
| @yanqiangmiffy                    | [InstructGLM](https://github.com/yanqiangmiffy/InstructGLM)                           | en/zh    | ChatGLM-6B                                 | ChatGLM based instruction-following model, fine-tuned on a variety of data sources, supports deepspeed accelerating and LoRA.                                                                                                                                                                                                                                                      |
| alibaba                           | [Wombat](https://github.com/GanjinZero/RRHF)                                          | en       | LLaMA                                      | a novel learning paradigm called RRHF, as an alternative of RLHF,  is proposed, which scores responses generated by<br /> different sampling policies and learns to align them with human preferences through ranking loss. And the performance<br />is comparable to RLHF, with less models used in the process.                                                                |
| @WuJunde                          | [alpaca-glassoff](https://github.com/WuJunde/alpaca-glassoff)                         | en       | LLaMA                                      | a mini image-acceptable Chat AI can run on your own laptop,  based on[stanford-alpaca](https://github.com/tatsu-lab/stanford_alpaca) and [alpaca-lora](https://github.com/tloen/alpaca-lora).                                                                                                                                                                                             |
| @JosephusCheung                   | [Guanaco](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)              | multi    | LLaMA-7B                                   | A Multilingual Instruction-Following Language Model.                                                                                                                                                                                                                                                                                                                                 |
| BlinkDL                           | [ChatRWKV](https://github.com/BlinkDL/ChatRWKV)                                       | en/zh    | [RWKV-LM](https://github.com/BlinkDL/RWKV-LM) | powered by RWKV (**100% RNN)**, Training sponsored by Stability EleutherAI.                                                                                                                                                                                                                                                                                                    |
| @FreedomIntelligence              | [LLM Zoo](https://github.com/FreedomIntelligence/LLMZoo)                              | multi    | BLOOMZ/LLaMA                               | a project that provides data, models, and evaluation benchmark for large language models.<br />model released: Phoenix, Chimera                                                                                                                                                                                                                                                     |
| SZU                               | [Linly](https://github.com/CVI-SZU/Linly)                                             | en/zh    | LLaMA                                      | **expand the Chinese vocabulary**, full fine-tuned models, largest LLaMA-based Chinese models, aggregation of Chinese instruction data, reproduceable details..                                                                                                                                                                                                               |
| @lamini-ai                        | [lamini](https://github.com/lamini-ai/lamini/)                                        | multi    | -                                          | data generator for generating instructions to train instruction-following LLMs.                                                                                                                                                                                                                                                                                                      |
| Stability-AI                      | [StableVicuna](https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot)       | en       | LLaMA                                      | a further instruction fine tuned and RLHF trained version of Vicuna v0 13b, with better performance than Vicuna.                                                                                                                                                                                                                                                                     |
| Hugging Face                      | [HuggingChat](https://huggingface.co/chat/)                                           | en       | LLaMA                                      | seems to be the first one available to access as a platform that appears similar to ChatGPT.                                                                                                                                                                                                                                                                                         |
| microsoft                         | [WizardLM](https://github.com/nlpxucan/WizardLM)                                      | en       | LLaMA-7B                                   | trained with 70k evolved instructions,[Evol-Instruct](https://github.com/nlpxucan/evol-instruct) is a novel method using LLMs instead of humans to automatically mass-produce<br /> open-domain instructions of various difficulty levels and skills range, to improve the performance of LLMs.                                                                                        |
| FDU                               | [OpenChineseLLaMA](https://github.com/OpenLMLab/OpenChineseLLaMA)                     | en/zh    | LLaMA-7B                                   | further pretrain LLaMA on Chinese data, improving LLaMA preformance on Chinese tasks.                                                                                                                                                                                                                                                                                               |
| @chenfeng357                      | [open-Chinese-ChatLLaMA](https://github.com/chenfeng357/open-Chinese-ChatLLaMA)       | en/zh    | LLaMA                                      | The complete training code of the open-source Chinese-Llama model, including the full process from pre-training instructing and RLHF.                                                                                                                                                                                                                                                |
| @FSoft-AI4Code                    | [CodeCapybara](https://github.com/FSoft-AI4Code/CodeCapybara)                         | en       | LLaMA                                      | Open Source LLaMA Model that Follow Instruction-Tuning for Code Generation.                                                                                                                                                                                                                                                                                                          |
| @mbzuai-nlp                       | [LaMini-LM](https://github.com/mbzuai-nlp/LaMini-LM)                                  | en       | LLaMA/Flan-T5 ...                          | A Diverse Herd of Distilled Models from Large-Scale Instructions.                                                                                                                                                                                                                                                                                                                    |
| NTU                               | [Panda](https://github.com/dandelionsllm/pandallm)                                    | en/zh    | LLaMA                                      | further pretraining on Chinese data, full-size of LLaMA models.                                                                                                                                                                                                                                                                                                                      |
| @hiyouga                          | [ChatGLM-Efficient-Tuning](https://github.com/hiyouga/ChatGLM-Efficient-Tuning)       | en/zh    | ChatGLM-6B                                 | efficient fine-tuning ChatGLM-6B with PEFT.                                                                                                                                                                                                                                                                                                                                          |
| IBM/CMU/MIT                       | [Dromedary](https://github.com/IBM/Dromedary)                                         | en       | LLaMA-65B                                  | Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision.                                                                                                                                                                                                                                                                                      |
| @melodysdreamj                    | [WizardVicunaLM](https://github.com/melodysdreamj/WizardVicunaLM)                     | multi    | Vicuna                                     | Wizard's dataset + ChatGPT's conversation extension + Vicuna's tuning method,<br /> achieving approximately 7% performance improvement over Vicuna.                                                                                                                                                                                                                                  |

# Multi-Modal

| contributor                               | project                                            | language | base model                              | main feature                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| ----------------------------------------- | -------------------------------------------------- | -------- | --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| BaihaiAIen/zh                             | [IDPChat](https://github.com/BaihaiAI/IDPChat)        | en/zh    | LLaMA-13B<br />Stable Diffusion         | Open Chinese multi-modal model, single GPU runnable, easy to deploy, UI provided.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| KAUST                                     | [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4) | en/zh    | LLaMA                                   | MiniGPT-4 aligns a frozen visual encoder from BLIP-2 with a frozen LLM, Vicuna, using just one projection layer,<br />and yields many emerging vision-language capabilities similar to those demonstrated in GPT-4.                                                                                                                                                                                                                                                                                                                                                                          |
| UW–Madison/MSR<br />/Columbia University | [LLaVA](https://github.com/haotian-liu/LLaVA)         | en       | LLaMA                                   | visual instruction tuning is proposed, towards building large language and vision models with GPT-4 level capabilities.                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| NUS/THU                                   | [VPGTrans](https://github.com/VPGTrans/VPGTrans)      | en       | LLaMA/OPT/<br />Flan-T5/BLIP-2<br />... | transferring VPG across LLMs to build VL-LLMs at significantly lower cost. The GPU hours<br /> can be reduced over 10 times and the training data can be reduced to around 10%.<br />Two novel VL-LLMs are released via VPGTrans, including **[VL-LLaMA](https://github.com/VPGTrans/VPGTrans#vl-llama)** and  **[VL-Vicuna](https://github.com/VPGTrans/VPGTrans#vl-vicuna)**.<br />**VL-LLaMA** is a multimodal version LLaMA by transferring the BLIP-2 OPT-6.7B to LLaMA via VPGTrans.<br />**VL-Vicuna** is a GPT-4-like multimodal chatbot, based on the Vicuna LLM. |
| CAS, etc                                  | [X-LLM](https://github.com/phellonchen/X-LLM)         | en/zh    | ChatGLM-6B                              | X-LLM converts multi-modalities (images, speech, videos) into foreign languages using X2L interfaces and feed them into<br /> a large Language Model (ChatGLM) to accomplish a Multimodal LLM, achieving impressive multimodal chat capabilities.                                                                                                                                                                                                                                                                                                                                             |
| NTU                                       | [Otter](https://github.com/Luodian/Otter)             | en       | OpenFlamingo                            | a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo),<br /> trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning.<br />Futhermore, optimize OpenFlamingo's implementation, democratizing the required<br /> training resources from 1x A100 GPU to 4x RTX-3090 GPUs.                                                                                                                                                                                                                                          |

# Data

## Pretrain Data

| contributor      | data                                                              | language | main feature                                               |
| ---------------- | ----------------------------------------------------------------- | -------- | ---------------------------------------------------------- |
| TogetherComputer | [RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data) | en       | An Open Source Recipe to Reproduce LLaMA training dataset. |

## Instruction Data

see [Alpaca-CoT data collection](https://github.com/PhoebusSi/Alpaca-CoT/blob/main/CN_README.md#3-%E6%95%B0%E6%8D%AE%E9%9B%86%E5%90%88-data-collection)

## Synthetic Data Generation

| contributor | method                                                                                              | main feature                                                                                                                                                                                                                                                                                                               |
| ----------- | --------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| UW, etc.    | [self-instruct](https://github.com/yizhongw/self-instruct)                                             | using the model's own generations to create a large collection of instructional data.                                                                                                                                                                                                                                      |
| @LiuHC0428  | [Reliable-Self-Instruction](https://github.com/LiuHC0428/LAW-GPT#%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94) | use ChatGPT to generate some questions and answers based on a given text.                                                                                                                                                                                                                                                  |
| PKU         | [Evol-Instruct](https://github.com/nlpxucan/evol-instruct)                                             | a novel method, proposed in[WizardLM](https://github.com/nlpxucan/WizardLM),  by using LLMs instead of humans to automatically mass-produce open-domain<br /> instructions of various difficulty levels and skills range, to improve the performance of LLMs.                                                               |
| KAUST, etc. | [CAMEL](https://github.com/lightaime/camel)                                                            | a novel communicative agent framework named*role-playing* is proposed, which involves using *inception prompting* to guide chat agents<br /> toward task completion while maintaining consistency with human intentions.<br />*role-playing* can be used to generate conversational data in a specific task/domain. |
| @chatarena  | [ChatArena](https://github.com/chatarena/chatarena)                                                    | a library that provides multi-agent language game environments and facilitates research about autonomous LLM agents and their social interactions.<br />it provides a flexible framework to define multiple players, environments and the interactions between them, based on Markov Decision Process.                     |

# Evaluation

| contributor      | method                                           | main feature                                                                                                                                                             |
| ---------------- | ------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| -                | human evalation                                  | -                                                                                                                                                                        |
| OpenAI           | GPT-4/ChatGPT                                    | -                                                                                                                                                                        |
| PKU/CMU/MSRA ... | [PandaLM](https://github.com/WeOpenML/PandaLM)      | Reproducible and Automated Language Model Assessment.                                                                                                                    |
| UCB              | [Chatbot Arena](https://github.com/lm-sys/FastChat) | Chat with two anonymous models side-by-side and vote for which one is better,<br /> then use the Elo rating system to calculate the relative performance of the models. |

# Framework/ToolKit/Platform

| contributor | project                                                                                    | main feature                                                                                                                                                                                                                                                 |
| ----------- | ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| CAS         | [Alpaca-CoT](https://github.com/PhoebusSi/Alpaca-CoT)                                         | extend CoT data to Alpaca to boost its reasoning ability.<br />aims at building an instruction finetuning (IFT) platform with extensive instruction collection (especially the CoT datasets)<br />and a unified interface for various large language models. |
| ColossalAI  | [ColossalChat](https://github.com/hpcaitech/ColossalAI/blob/main/applications/Chat/README.md) | An open-source low cost solution for cloning[ChatGPT](https://openai.com/blog/chatgpt/) with a complete RLHF pipeline.                                                                                                                                          |
| microsoft   | [deepspeed-chat](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat)     | Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales.                                                                                                                                                                                |
| LAION-AI    | [Open Assistant](https://github.com/LAION-AI/Open-Assistant)                                  | a project meant to give everyone access to a great chat based large language model.                                                                                                                                                                          |
| HKUST       | [LMFlow](https://github.com/OptimalScale/LMFlow)                                              | an extensible, convenient, and efficient toolbox for finetuning large machine learning models,<br /> designed to be user-friendly, speedy and reliable, and accessible to the entire community.                                                              |
| UCB         | [EasyLM](https://github.com/young-geng/EasyLM)                                                | EasyLM is a one stop solution for pre-training, finetuning, evaluating and serving LLMs in JAX/Flax.<br /> EasyLM can scale up LLM training to hundreds of TPU/GPU accelerators by leveraging JAX's pjit functionality.                                      |
| @CogStack   | [OpenGPT](https://github.com/CogStack/opengpt)                                                | A framework for creating grounded instruction based datasets and training conversational domain expert Large Language Models (LLMs).                                                                                                                         |
| HugAILab    | [HugNLP](https://github.com/HugAILab/HugNLP)                                                  | a unified and comprehensive NLP library based on HuggingFace Transformer.                                                                                                                                                                                    |

# Alignment

| contributor | method                                                                                   | used in                                                   | main feature                                                                                                                                                                                                                                                                                                         |
| ----------- | ---------------------------------------------------------------------------------------- | --------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| -           | [IFT](https://arxiv.org/pdf/2109.01652.pdf)                                                 | [ChatGPT](https://openai.com/blog/chatgpt/)                  | Instruction Fine-Tuning.                                                                                                                                                                                                                                                                                            |
| -           | [RLHF](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback)            | [ChatGPT](https://openai.com/blog/chatgpt/)                  | RL from Human Feedback.                                                                                                                                                                                                                                                                                             |
| Anthropic   | [RLAIF](https://arxiv.org/abs/2212.08073)                                                   | [Claude](https://www.anthropic.com/index/introducing-claude) | RL from AI Feedback.                                                                                                                                                                                                                                                                                                 |
| alibaba     | [RRHF](https://arxiv.org/pdf/2304.05302v1.pdf)                                              | [Wombat](https://github.com/GanjinZero/RRHF)                 | a novel learning paradigm called RRHF, as an alternative of RLHF,  is proposed, which scores responses generated by<br />different sampling policies and learns to align them with human preferences through ranking loss. And the performance<br />is comparable to RLHF, with less models used in the process. |
| HKUST       | [RAFT](https://optimalscale.github.io/LMFlow/examples/raft.html)                            | -                                                         | RAFT is a new alignment algorithm, which is more efficient than conventional (PPO-based) RLHF.                                                                                                                                                                                                                      |
| IBM/CMU/MIT | [SELF-ALIGN](https://arxiv.org/abs/2305.03047)                                              | [Dromedary](https://github.com/IBM/Dromedary)                | combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision.                                                                                                                                                                             |
| PKU         | [CVA](https://github.com/PKU-Alignment/safe-rlhf#constrained-value-alignment-via-safe-rlhf) | [Beaver](https://github.com/PKU-Alignment/safe-rlhf)         | Constrained Value Alignment via Safe RLHF.                                                                                                                                                                                                                                                                           |

# Multi-Language

## vocabulary expansion

according to the official [FAQ](https://github.com/facebookresearch/llama/blob/main/FAQ.md#4-other-languages) in LLaMA repo, there's not many tokens other than latin languages, so one of the efforts is to expand the vocabulary, some works are shown below:

| contributor    | model/project                                                      | language | base model     | main feature                                                                                                                                                                                   |
| -------------- | ------------------------------------------------------------------ | -------- | --------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| @ymcui         | [Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) | zh       | LLaMA           |                                                                                                                                                                                                |
| SZU            | [Linly](https://github.com/CVI-SZU/Linly)                             | en/zh    | LLaMA           | full-size LLaMA, further pretrained on Chineses Corpus.                                                                                                                                        |
| @Neutralzz     | [BiLLa](https://github.com/Neutralzz/BiLLa)                           | en/zh    | LLaMA-7B        | further pretrained on[Wudao](https://www.sciencedirect.com/science/article/pii/S2666651021000152)、[PILE](https://arxiv.org/abs/2101.00027)、[WMT](https://www.statmt.org/wmt22/translation-task.html). |
| @pengxiao-song | [LaWGPT](https://github.com/pengxiao-song/LaWGPT)                     | zh       | zhLLaMA/ChatGLM | expand the vocab with Chinese legal terminologies, instruction fine-tuned on data generated using self-instruct.                                                                              |

# Efficient Training/Fine-Tuning

| contributor                              | method                                                    | main feature                                                                                                                                                                                                                                                                                            |
| ---------------------------------------- | --------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| microsoft                                | [LoRA](https://arxiv.org/abs/2106.09685)                     | Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices<br /> into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks.                                            |
| stanford                                 | [Prefix Tuning](https://aclanthology.org/2021.acl-long.353/) | a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen<br /> and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix.                                                                      |
| THU                                      | [P-Tuning](https://arxiv.org/abs/2103.10385)                 | P-tuning leverages few continuous free parameters to serve as prompts fed as the input to the pre-trained language models.<br />We then optimize the continuous prompts using gradient descent as an alternative to discrete prompt searching.                                                        |
| THU/BAAI/<br />Shanghai Qi Zhi Institute | [P-Tuning v2](https://arxiv.org/pdf/2110.07602.pdf)          | a novel empirical finding that properly optimized prompt tuning can be comparable to fine-tuning universally across various model scales and NLU tasks.<br />Technically, P-tuning v2 is not conceptually novel. It can be viewed as an optimized and adapted implementation of Deep Prompt Tuning. |
| Google                                   | [Prompt Tuning](https://arxiv.org/abs/2104.08691)            | a simple yet effective mechanism for learning "soft prompts" to condition frozen language models to perform specific downstream tasks.<br />Prompt Tuning can be seen as a simplification of "prefix tuning".                                                                                         |
| GT/Princeton/microsoft                   | [AdaLoRA](https://arxiv.org/abs/2303.10512)                  | adaptively allocates the parameter budget among weight matrices according to their importance score.<br /> In particular, AdaLoRA parameterizes the incremental updates in the form of singular value decomposition.                                                                                    |

acknowledgement: [HuggingFace Peft](https://github.com/huggingface/peft)

# Low-Cost Inference

| contributor                      | project                                                        | main feature                                                                                                                                                                                                                                                                                                                          |
| -------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| @ggerganov                       | [llama.cpp](https://github.com/ggerganov/llama.cpp)               | c/cpp implementation for llama and some other models, using quantization.                                                                                                                                                                                                                                                            |
| @NouamaneTazi                    | [bloomz.cpp](https://github.com/NouamaneTazi/bloomz.cpp)          | C++ implementation for BLOOM inference.                                                                                                                                                                                                                                                                                               |
| @mlc-ai                          | [MLC LLM](https://github.com/mlc-ai/mlc-llm)                      | a universal solution that allows any language models to be deployed natively on a diverse set of hardware backends and native applications,<br />plus a productive framework for everyone to further optimize model performance for their own use cases.                                                                           |
| alibaba                          | [ChatGLM-MNN](https://github.com/wangzhaode/ChatGLM-MNN)          | converts the ChatGLM-6B model to MNN and performs inference using C++.                                                                                                                                                                                                                                                               |
| Jittor                           | [JittorLLMs](https://github.com/Jittor/JittorLLMs)                | Significantly reduce hardware costs (by 80%), currently known as the lowest-cost deployment library, supports multiple platforms.                                                                                                                                                                                                     |
| OpenBMB                          | [BMInf](https://github.com/OpenBMB/BMInf)                         | BMInf supports running models with more than 10 billion parameters on a single NVIDIA GTX 1060 GPU in its minimum requirements.<br /> In cases where the GPU memory supports the large model inference (such as V100 or A100),<br /> BMInf still has a significant performance improvement over the existing PyTorch implementation. |
| hpcaitech                        | [EnergonAI](https://github.com/hpcaitech/EnergonAI)               | With tensor parallel operations, pipeline parallel wrapper, distributed checkpoint loading, and customized CUDA kernel,<br /> EnergonAI can enable efficient parallel inference for larges-scale models.                                                                                                                              |
| MegEngine                        | [InferLLM](https://github.com/MegEngine/InferLLM)                 | a lightweight LLM model inference framework that mainly references and borrows from[the llama.cpp project](https://github.com/ggerganov/llama.cpp).<br /> llama.cpp puts almost all core code and kernels in a single file and use a large number of macros, making it difficult for developers to read and modify.                      |
| @saharNooby                      | [rwkv.cpp](https://github.com/saharNooby/rwkv.cpp)                | a port of[BlinkDL/RWKV-LM](https://github.com/BlinkDL/RWKV-LM) to [ggerganov/ggml](https://github.com/ggerganov/ggml).                                                                                                                                                                                                                      |
| FMInference                      | [FlexGen](https://github.com/FMInference/FlexGen)                 | FlexGen is a high-throughput generation engine for running large language models with limited GPU memory.<br /> FlexGen allows**high-throughput** generation by IO-efficient offloading, compression, and  **large effective batch sizes** .                                                                              |
| huggingface<br />bigcode-project | [starcoder.cpp](https://github.com/bigcode-project/starcoder.cpp) | C++ implemention for 💫 StarCoder inference using the[ggml](https://github.com/ggerganov/ggml) library.                                                                                                                                                                                                                                |

# Satefy

| contributor | method                                                    | main feature                                                            |
| ----------- | --------------------------------------------------------- | ----------------------------------------------------------------------- |
| thu-coai    | [Safety-Prompts](https://github.com/thu-coai/Safety-Prompts) | Chinese safety prompts for evaluating and improving the safety of LLMs. |

# Input Length Extrapolation

| contributor      | method                                                          | main feature                                                                                                                                                                                                                                                                             |
| ---------------- | --------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| UW, etc.         | [ALiBi](https://github.com/ofirpress/attention_with_linear_biases) | Instead of adding position embeddings at the bottom of the transformer stack,<br /> ALiBi adds a linear bias to each attention score, allowing the model to be trained on,<br /> for example, 1024 tokens, and then do inference on 2048 (or much more) tokens without any finetuning. |
| DeepPavlov, etc. | [RMT](https://arxiv.org/abs/2304.11062)                            | use a recurrent memory to extend the context length.                                                                                                                                                                                                                                    |
| bytedance        | [SCM](https://arxiv.org/abs/2304.11062)                            | unleash infinite-length input capacity for large-scale language models.                                                                                                                                                                                                                  |
| BlinkDL          | [RWKV-LM](https://github.com/BlinkDL/RWKV-LM)                      | pure RNN.                                                                                                                                                                                                                                                                                |
