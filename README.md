# gopen source ChatGPT and beyond

On the road to implement open-source ChatGPT-like models and beyond.

Since the accidental leak of LLaMA model weights, and the impressive performance of Stanford Alpaca, which is trained on LLaMA using data generated by GPT-3 api with the self-instruct technique, the open-source community has been excited about the promising future of reproducing ChatGPT in an open way.

This repo aims at recording this process, and providing an overview of how to get involved.

Including: base models, technologies, data, domain models, training pipelines, speed up techniques, multi-language, multi-modal, and more to go.

thanks [@FunnySaltyFish](https://github.com/FunnySaltyFish) for the [website version](https://llm.best/), here is the [code](https://github.com/FunnySaltyFish/best_llm).

Any contribution to this project and the website is appreciated! (we are short of hands ...)

# Table of Contents

- [Base Models](#base-models)
- [Domain Models](#domain-models)
- [General Domain Instruction Models](#general-domain-instruction-models)
- [Alternatives To Transformer](#alternatives-to-transformer)
- [Multi-Modal](#multi-modal)
- [Data](#data)
  - [Pretrain Data](#pretrain-data)
  - [Instruction Data](#instruction-data)
  - [Synthetic Data Generation](#synthetic-data-generation)
- [Evaluation](#evaluation)
  - [Benchmark](#enchmark)
  - [LeaderBoard](#leaderboard)
- [Framework/ToolKit/Platform](#frameworktoolkitplatform)
- [Alignment](#alignment)
- [Multi-Language](#multi-language)
  - [vocabulary expansion](#vocabulary-expansion)
- [Efficient Training/Fine-Tuning](#efficient-trainingfine-tuning)
- [Low-Cost Inference](#low-cost-inference)
- [Prompting](#prompting)
- [Safety](#safety)
- [Truthfulness](#truthfulness)
- [Exceeding Context Window](#exceeding-context-window)
- [Knowledge Editing](#knowledge-editing)
  - [Implementations](#implementations)
- [External Knowledge](#external-knowledge)
- [External Tools](#external-tools)
  - [Using Existing Tools](using-existing-tools)
  - [Make New Tools](make-new-tools)
- [Agent](#agent)
- [LLMs as XXX](#llms-as-xxx)
- [Similar Collections](#similar-collections)

# Base Models

| contributor                                | model/project                                                                                                               | license                                                                                                                                                                                                                                                                                           | language | main feature                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| ------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Meta                                       | [LLaMA/LLaMA2](https://github.com/facebookresearch/llama)                                                                      |                                                                                                                                                                                                                                                                                                   | multi    | LLaMA-13B outperforms GPT-3(175B) and LLaMA-65B is competitive to PaLM-540M.<br />Base model for most follow-up works.                                                                                                                                                                                                                                                                                                                                                      |
| HuggingFace-BigScience                     | [BLOOM](https://huggingface.co/bigscience/bloom)                                                                               |                                                                                                                                                                                                                                                                                                   | multi    | an autoregressive Large Language Model (LLM) trained by HuggingFace BigScience.                                                                                                                                                                                                                                                                                                                                                                                             |
| HuggingFace-BigScience                     | [BLOOMZ](https://huggingface.co/bigscience/bloomz)                                                                             |                                                                                                                                                                                                                                                                                                   | multi    | instruction-finetuned version of BLOOM & mT5 pretrained multilingual language models on crosslingual task mixture.                                                                                                                                                                                                                                                                                                                                                         |
| EleutherAI                                 | [GPT-J](https://huggingface.co/EleutherAI/gpt-j-6b)                                                                            |                                                                                                                                                                                                                                                                                                   | en       | transformer model trained using Ben Wang's[Mesh Transformer JAX](https://github.com/kingoflolz/mesh-transformer-jax/).                                                                                                                                                                                                                                                                                                                                                         |
| Meta                                       | [OPT](https://huggingface.co/facebook/opt-66b)                                                                                 |                                                                                                                                                                                                                                                                                                   | en       | Open Pre-trained Transformer Language Models, aim in developing this suite of OPT models is to enable reproducible<br /> and responsible research at scale, and to bring more voices to the table in studying the impact of these LLMs.                                                                                                                                                                                                                                    |
| [Cerebras Systems](https://www.cerebras.net/) | [Cerebras-GPT](https://huggingface.co/cerebras/Cerebras-GPT-13B)                                                               |                                                                                                                                                                                                                                                                                                   | en       | Pretrained LLM, GPT-3 like, Commercially available, efficiently trained on the[Andromeda](https://www.cerebras.net/andromeda/) AI supercomputer,<br />trained in accordance with[Chinchilla scaling laws](https://arxiv.org/abs/2203.15556) (20 tokens per model parameter) which is compute-optimal.                                                                                                                                                                            |
| EleutherAI                                 | [pythia](https://github.com/EleutherAI/pythia)                                                                                 |                                                                                                                                                                                                                                                                                                   | en       | combine interpretability analysis and scaling laws to understand how knowledge develops<br />and evolves during training in autoregressive transformers.                                                                                                                                                                                                                                                                                                                    |
| Stability-AI                               | [StableLM](https://github.com/Stability-AI/StableLM)                                                                           |                                                                                                                                                                                                                                                                                                   | en       | Stability AI Language Models                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| FDU                                        | [MOSS](https://github.com/OpenLMLab/MOSS)                                                                                      |                                                                                                                                                                                                                                                                                                   | en/zh    | An open-source tool-augmented conversational language model from Fudan University.                                                                                                                                                                                                                                                                                                                                                                                          |
| ssymmetry & FDU                            | [BBT-2](https://bbt.ssymmetry.com/)                                                                                            |                                                                                                                                                                                                                                                                                                   | zh       | 12B open-source LM.                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| @mlfoundations                             | [OpenFlamingo](https://github.com/mlfoundations/open_flamingo)                                                                 |                                                                                                                                                                                                                                                                                                   | en       | An open-source framework for training large multimodal models.                                                                                                                                                                                                                                                                                                                                                                                                              |
| EleutherAI                                 | [GPT-NeoX-20B](https://huggingface.co/EleutherAI/gpt-neox-20b)                                                                 |                                                                                                                                                                                                                                                                                                   | en       | Its architecture intentionally resembles that of GPT-3, and is almost identical to that of[GPT-J- 6B](https://huggingface.co/EleutherAI/gpt-j-6B).                                                                                                                                                                                                                                                                                                                             |
| UCB                                        | [OpenLLaMA](https://github.com/openlm-research/open_llama)                                                                     | Apache-2.0                                                                                                                                                                                                                                                                                        | en       | An Open Reproduction of LLaMA.                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| MosaicML                                   | [MPT](https://github.com/mosaicml/llm-foundry)                                                                                 | Apache-2.0                                                                                                                                                                                                                                                                                        | en       | MPT-7B is a GPT-style model, and the first in the MosaicML Foundation Series of models.<br /> Trained on 1T tokens of a MosaicML-curated dataset, MPT-7B is open-source,<br /> commercially usable, and equivalent to LLaMa 7B on evaluation metrics.                                                                                                                                                                                                                       |
| TogetherComputer                           | [RedPajama-INCITE-Base-3B-v1](https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1)                             | Apache-2.0                                                                                                                                                                                                                                                                                        | en       | A 2.8B parameter pretrained language model, pretrained on[RedPajama-Data-1T](https://huggingface.co/models?dataset=dataset:togethercomputer/RedPajama-Data-1T),<br /> together with an [Instruction-tuned Version](https://huggingface.co/togethercomputer/RedPajama-INCITE-Instruct-3B-v1) and a [Chat Version](https://huggingface.co/togethercomputer/RedPajama-INCITE-Chat-3B-v1).                                                                                             |
| Lightning-AI                               | [Lit-LLaMA](https://github.com/Lightning-AI/lit-llama)                                                                         | Apache-2.0                                                                                                                                                                                                                                                                                        | -        | Independent implementation of[LLaMA](https://github.com/facebookresearch/llama) that is fully open source under the **Apache 2.0 license.**                                                                                                                                                                                                                                                                                                                              |
| @conceptofmind                             | [PaLM](https://github.com/conceptofmind/PaLM)                                                                                  | MIT License                                                                                                                                                                                                                                                                                       | en       | An open-source implementation of Google PaLM models.                                                                                                                                                                                                                                                                                                                                                                                                                       |
| [TII](https://www.tii.ae/)                    | [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b)                                                                           | [TII Falcon LLM License](https://huggingface.co/tiiuae/falcon-7b/blob/main/LICENSE.txt)                                                                                                                                                                                                              | en       | a 7B parameters causal decoder-only model built by[TII](https://www.tii.ae/) and trained on 1,500B tokens of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) enhanced with curated corpora.                                                                                                                                                                                                                                                                |
| [TII](https://www.tii.ae/)                    | [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b)                                                                         | [TII Falcon LLM License](https://huggingface.co/tiiuae/falcon-7b/blob/main/LICENSE.txt)                                                                                                                                                                                                              | multi    | a 40B parameters causal decoder-only model built by[TII](https://www.tii.ae/) and trained on 1,000B tokens of [RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb) enhanced with curated corpora.                                                                                                                                                                                                                                                               |
| TigerResearch                              | [TigerBot](https://github.com/TigerResearch/TigerBot)                                                                          | Apache-2.0                                                                                                                                                                                                                                                                                        | en/zh    | a multi-language and multitask LLM.                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| BAAI                                       | [Aquila](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila) / [Aquila2](https://github.com/FlagAI-Open/Aquila2) | [BAAI_Aquila_Model_License](https://github.com/FlagAI-Open/FlagAI/blob/master/BAAI_Aquila_Model_License.pdf)                                                                                                                                                                                         | en/zh    | The Aquila language model inherits the architectural design advantages of GPT-3 and LLaMA, replacing a batch of more efficient underlying<br /> operator implementations and redesigning the tokenizer for Chinese-English bilingual support.                                                                                                                                                                                                                               |
| OpenBMB                                    | [CPM-Bee](https://github.com/OpenBMB/CPM-Bee)                                                                                  | [通用模型许可协议-来源说明-宣传限制-商业授权](https://github.com/OpenBMB/General-Model-License/blob/main/%E9%80%9A%E7%94%A8%E6%A8%A1%E5%9E%8B%E8%AE%B8%E5%8F%AF%E5%8D%8F%E8%AE%AE-%E6%9D%A5%E6%BA%90%E8%AF%B4%E6%98%8E-%E5%AE%A3%E4%BC%A0%E9%99%90%E5%88%B6-%E5%95%86%E4%B8%9A%E6%8E%88%E6%9D%83.md) | en/zh    | **CPM-Bee** is a fully open-source, commercially-usable Chinese-English bilingual base model with a capacity of ten billion parameters.<br />And has been pre-trained on an extensive corpus of trillion-scale tokens.                                                                                                                                                                                                                                               |
| Baichuan                                   | [baichuan-7B](https://github.com/baichuan-inc/baichuan-7B)                                                                     | Apache-2.0                                                                                                                                                                                                                                                                                        | en/zh    | It has achieved the best performance among models of the same size on standard<br /> Chinese and English authoritative benchmarks (C-EVAL, MMLU, etc).                                                                                                                                                                                                                                                                                                                      |
| Tencent                                    | [lyraChatGLM](https://huggingface.co/TMElyralab/lyraChatGLM)                                                                   | MIT License                                                                                                                                                                                                                                                                                       | en/zh    | To the best of our knowledge, it is the**first accelerated version of ChatGLM-6B**.<br />The inference speed of lyraChatGLM has achieved **300x** acceleration upon the early original version.<br /> We are still working hard to further improve the performance.                                                                                                                                                                                             |
| SalesForce                                 | [XGen](https://github.com/salesforce/xgen)                                                                                     | Apache-2.0                                                                                                                                                                                                                                                                                        | multi    | Salesforce open-source LLMs with 8k sequence length                                                                                                                                                                                                                                                                                                                                                                                                                         |
| Shanghai AI Lab                            | [InternLM](https://github.com/InternLM/InternLM)                                                                               | Apache-2.0                                                                                                                                                                                                                                                                                        | en/zh    | InternLM has open-sourced a 7 billion parameter base model and a chat model tailored for practical scenarios. The model has the following characteristics:<br />It leverages trillions of high-quality tokens for training to establish a powerful knowledge base.<br />It supports an 8k context window length, enabling longer input sequences and stronger reasoning capabilities.<br />It provides a versatile toolset for users to flexibly build their own workflows. |
| xverse-ai                                  | [XVERSE-13B](https://github.com/xverse-ai/XVERSE-13B)                                                                          | Apache-2.0                                                                                                                                                                                                                                                                                        | multi    | A multilingual large language model developed by XVERSE Technology Inc.                                                                                                                                                                                                                                                                                                                                                                                                     |
| Writer                                     | [palmyra](https://huggingface.co/Writer/palmyra-base)                                                                          | Apache-2.0                                                                                                                                                                                                                                                                                        | en       | extremely powerful while being extremely fast. This model excels at many nuanced tasks<br /> such as sentiment classification and summarization.                                                                                                                                                                                                                                                                                                                            |
| Mistral AI                                 | [Mistral](https://huggingface.co/mistralai/Mistral-7B-v0.1)                                                                    | Apache-2.0                                                                                                                                                                                                                                                                                        | en       | Mistral 7B is a 7.3B parameter model that:<br />1. Outperforms Llama 2 13B on all benchmarks<br />2. Outperforms Llama 1 34B on many benchmarks<br />3. Approaches CodeLlama 7B performance on code, while remaining good at English tasks<br />4. Uses Grouped-query attention (GQA) for faster inference<br />5. Uses Sliding Window Attention (SWA) to handle longer sequences at smaller cost                                                                           |

# Domain Models

| contributor                        | model                                                                                                              | domain          | language | base model                                                                                                  | main feature                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| ---------------------------------- | ------------------------------------------------------------------------------------------------------------------ | --------------- | -------- | ------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| UT Southwestern/<br />UIUC/OSU/HDU | [ChatDoctor](https://github.com/Kent0n-Li/ChatDoctor)                                                                 | medical         | en       | LLaMA                                                                                                        | Maybe the first domain-specific chat model tuned on LLaMA.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| Cambridge                          | [Visual Med-Alpaca](https://github.com/cambridgeltl/visual-med-alpaca)                                                | biomedical      | en       | LLaMA-7B                                                                                                     | a multi-modal foundation model designed specifically for the biomedical domain.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| HIT                                | [BenTsao](https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese) / [ChatGLM-Med](https://github.com/SCIR-HI/Med-ChatGLM) | medical         | zh       | LLaMA/ChatGLM                                                                                                | fine-tuned with Chinese medical knowledge dataset, which is generated by using gpt3.5 api.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| ShanghaiTech, etc.                 | [DoctorGLM](https://github.com/xionghonglin/DoctorGLM)                                                                | medical         | en/zh    | ChatGLM-6B                                                                                                   | Chinese medical consultation model fine-tuned on ChatGLM-6B.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| THU AIR                            | [BioMedGPT-1.6B](https://github.com/BioFM/OpenBioMed)                                                                 | biomedical      | en/zh    | -                                                                                                            | a pre-trained multi-modal molecular foundation model with 1.6B parameters that associates 2D molecular graphs with texts.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| @LiuHC0428                         | [LawGPT_zh](https://github.com/LiuHC0428/LAW-GPT)                                                                     | legal           | zh       | ChatGLM-6B                                                                                                   | a general model in Chinese legal domain, trained on data generated via Reliable-Self-Instruction.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| SJTU                               | [MedicalGPT-zh](https://github.com/MediaBrain-SJTU/MedicalGPT-zh)                                                     | medical         | zh       | ChatGLM-6B                                                                                                   | a general model in Chinese medical domain, a diverse data generated via self-instruct.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| SJTU                               | [PMC-LLaMA](https://github.com/chaoyi-wu/PMC-LLaMA)                                                                   | medical         | zh       | LLaMA                                                                                                        | Continue Training LLaMA on Medical Papers.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| HuggingFace                        | [StarCoder](https://github.com/bigcode-project/starcoder)                                                             | code generation | en       | -                                                                                                            | a language model (LM) trained on source code and natural language text. Its training data incorporates more than<br /> 80 different programming languages as well as text extracted from GitHub issues and commits and from notebooks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| @CogStack                          | [NHS-LLM](https://github.com/CogStack/opengpt#nhs-llm)                                                                | medical         | en       | not clear                                                                                                    | A conversational model for healthcare trained using[OpenGPT](https://github.com/CogStack/opengpt).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| @pengxiao-song                     | [LaWGPT](https://github.com/pengxiao-song/LaWGPT)                                                                     | legal           | zh       | LLaMA/ChatGLM                                                                                                | expand the vocab with Chinese legal terminologies, instruction fine-tuned on data generated using self-instruct.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| Duxiaoman                          | [XuanYuan](https://github.com/Duxiaoman-DI/XuanYuan)                                                                  | finance         | zh       | BLOOM-176B                                                                                                   | A Large Chinese Financial Chat Model with Hundreds of Billions Parameters.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| CUHK                               | [HuatuoGPT](https://github.com/FreedomIntelligence/HuatuoGPT)                                                         | medical         | zh       | not clear                                                                                                    | HuatuoGPT, a large language model (LLM) trained on a vast Chinese medical corpus. Our objective with HuatuoGPT is<br /> to construct a more professional ‘ChatGPT’ for medical consultation scenarios.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| PKU                                | [Lawyer LLaMA](https://github.com/AndrewZhe/lawyer-llama)                                                             | legal           | zh       | LLaMA                                                                                                        | continue pretraining on Chinese legal data, insturction tuned on legal exams and legal consulting qa pairs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| THU                                | [LexiLaw](https://github.com/CSHaitao/LexiLaw)                                                                        | legal           | zh       | ChatGLM-6B                                                                                                   | trained on a mixture of general data ([BELLE](https://github.com/LianjiaTech/BELLE) 1.5M) and legal data                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| THU, etc.                          | [taoli](https://github.com/blcuicall/taoli)                                                                           | education       | zh       | LLaMA                                                                                                        | A large model for international Chinese education. It extends specific vocabulary on the base model,<br /> and uses the domain's proprietary data set for instruction fine-tuning.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| NUS                                | [Goat](https://github.com/liutiedong/goat)                                                                            | arithmetic      | en       | LLaMA                                                                                                        | a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks.<br /> Fine-tuned on a synthetically generated dataset, Goat achieves state-ofthe-art performance on BIG-bench arithmetic sub-task.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| CU/NYU                             | [FinGPT](https://github.com/AI4Finance-Foundation/FinGPT)                                                             | finance         | en       | -                                                                                                            | an end-to-end open-source framework for financial large language models (FinLLMs).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| microsoft                          | [WizardCoder](https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder)                                             | code generation | en       | StarCoder                                                                                                    | trained with**78k** evolved code instructions. surpasses  **Claude-Plus (+6.8)** , **Bard (+15.3)** and **InstructCodeT5+ (+22.3)** on the [HumanEval Benchmarks](https://github.com/openai/human-eval).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| UCAS                               | [Cornucopia](https://github.com/jerry1993-tech/Cornucopia-LLaMA-Fin-Chinese)                                          | finance         | zh       | LLaMA                                                                                                        | finetune LLaMA on Chinese financial knowledge,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| PKU                                | [ChatLaw](https://github.com/PKU-YuanGroup/ChatLaw)                                                                   | legal           | zh       | [Ziya](https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-Pretrain-v1) / [Anima](https://github.com/lyogavin/Anima) | Chinese legal domain model.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| @michael-wzhu                      | [ChatMed](https://github.com/michael-wzhu/ChatMed)                                                                    | medical         | zh       | LLaMA                                                                                                        | Chinese medical LLM based on LLaMA-7B.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| SCUT                               | [SoulChat](https://github.com/scutcyr/SoulChat)                                                                       | mental health   | zh       | ChatGLM-6B                                                                                                   | Chinese dialogue LLM in mental health domain, based on ChatGLM-6B.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| @shibing624                        | [MedicalGPT](https://github.com/shibing624/MedicalGPT)                                                                | medical         | zh       | ChatGLM-6B                                                                                                   | Training Your Own Medical GPT Model with ChatGPT Training Pipeline.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| BJTU                               | [TransGPT](https://github.com/DUOMO/TransGPT)                                                                         | transportation  | zh       | LLaMA-7B                                                                                                     | Chinese transportation model.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| BAAI                               | [AquilaCode](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila/Aquila-code)                           | code generation | multi    | Aquila                                                                                                       | AquilaCode-multi is a multi-language model that supports high-accuracy code generation for various programming languages, including Python/C++/Java/Javascript/Go, etc.<br /> It has achieved impressive results in HumanEval (Python) evaluation, with Pass@1, Pass@10, and Pass@100 scores of 26/45.7/71.6, respectively. In the HumanEval-X<br /> multi-language code generation evaluation, it significantly outperforms other open-source models with similar parameters (as of July 19, 2023).<br />AquilaCode-py, on the other hand, is a single-language Python version of the model that focuses on Python code generation. <br />It has also demonstrated excellent performance in HumanEval evaluation, with Pass@1, Pass@10, and Pass@100 scores of 28.8/50.6/76.9 (as of July 19, 2023). |
| Meta                               | [CodeLLaMA](https://github.com/facebookresearch/codellama)                                                            | code generation | multi    | LLaMA-2                                                                                                      | a family of large language models for code based on[Llama 2](https://github.com/facebookresearch/llama) providing state-of-the-art performance among open models, infilling capabilities,<br /> support for large input contexts, and zero-shot instruction following ability for programming tasks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| UNSW, etc                          | [Darwin](https://github.com/MasterAI-EAM/Darwin)                                                                      | natural science | en       | LLaMA-7B                                                                                                     | the first open-source LLM for natural science, mainly in physics, chemistry and material science.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| alibaba                            | [EcomGPT](https://github.com/Alibaba-NLP/EcomGPT)                                                                     | e-commerce      | en/zh    | BLOOMZ                                                                                                       | An Instruction-tuned Large Language Model for E-commerce.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| TIGER-AI-Lab                       | [MAmmoTH](https://github.com/TIGER-AI-Lab/MAmmoTH)                                                                    | math            | en       | LLaMA2/CodeLLaMA                                                                                             | a series of open-source large language models (LLMs) specifically tailored for general math problem-solving. The MAmmoTH models are trained on MathInstruct,<br /> a meticulously curated instruction tuning dataset that is lightweight yet generalizable. MathInstruct is compiled from 13 math rationale datasets,<br /> six of which are newly curated by this work. It uniquely focuses on the hybrid use of chain-of-thought (CoT) and program-of-thought (PoT) rationales,<br /> and ensures extensive coverage of diverse mathematical fields.                                                                                                                                                                                                                                                |
| SJTU                               | [abel](https://github.com/GAIR-NLP/abel)                                                                              | math            | en       | LLaMA2                                                                                                       | We propose**Parental Oversight*** , A ***Babysitting Strategy*** for Supervised Fine-tuning, `Parental Oversight` is not limited to any specific data processing method. Instead, it defines the data processing philosophy that should guide supervised fine-tuning in the era of Generative AI GAI).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| FDU                                | [DISC-LawLLM](https://github.com/FudanDISC/DISC-LawLLM)                                                               | legal           | zh       | Baichuan-13B                                                                                                 | FudanDISC has released DISC-LawLLM, a Chinese intelligent legal system driven by a large language model.<br /> The system can provide various legal services for different user groups. In addition, DISC-Law-Eval is constructed to evaluate the large legal language model from both objective and subjective aspects.<br /> The model has obvious advantages compared with the existing large legal models.<br />The team also made available a high-quality Supervised fine-tuning (SFT) dataset of 300,000, DISC-Law-SFT.                                                                                                                                                                                                                                                                        |
| HKU, etc                           | [ChatPsychiatrist](https://github.com/EmoCareAI/ChatPsychiatrist)                                                     | mental health   | en       | LLaMA-7B                                                                                                     | This repo open-sources the Instruct-tuned LLaMA-7B model that has been fine-tuned with counseling domian instruction data.<br /> To construct our 8K size instruct-tuning dataset, we collected real-world counseling dialogue examples and employed GPT-4 as an extractor and filter.<br /> In addition, we have introduced a comprehensive set of metrics, specifically tailored to the LLM+Counseling domain, by incorporating counseling domain evaluation criteria.<br /> These metrics enable the assessment of performance in generating language content that involves multi-dimensional counseling skills.                                                                                                                                                                                   |
| CAS                                | [StarWhisper](https://wisemodel.cn/models/LiYuYang/StarWhisper)                                                       | astronomical    | zh       | -                                                                                                            | StarWhisper, a large astronomical model, significantly improves the reasoning logic and integrity of the model through the fine-tuning of astrophysical corpus labeled by experts,<br /> logical long text training, and direct preference optimization. In the CG-Eval jointly published by the Keguei AI Research Institute and LanguageX AI Lab, it reached the second place overall,<br /> just below GPT-4, and its mathematical reasoning and astronomical capabilities are close to or exceed the GPT 3.5 Turbo.                                                                                                                                                                                                                                                                               |
| PKU, etc                           | [CodeShell](https://github.com/WisdomShell/codeshell)                                                                 | code generation | en/zh    | -                                                                                                            | CodeShell is a code large language model (LLM) developed jointly by the[Knowledge Computing Lab at Peking University](http://se.pku.edu.cn/kcl/) and the AI team of Sichuan Tianfu Bank. CodeShell has 7 billion parameters,<br /> was trained on 500 billion tokens, and has a context window length of 8192. On authoritative code evaluation benchmarks (HumanEval and MBPP), CodeShell achieves the best performance for models of its scale.                                                                                                                                                                                                                                                                                                                                                        |
| FDU                                | [DISC-FinLLM](https://github.com/FudanDISC/DISC-FinLLM)                                                               | finance         | zh       | Baichuan-13B-Chat                                                                                            | DISC-FinLLM is a large language model in the financial field. It is a multi-expert intelligent financial system composed of four modules for different financial scenarios: financial consulting,<br /> financial text analysis, financial calculation, and financial knowledge retrieval and question answering.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |

some medical models: [here](https://mp.weixin.qq.com/s/c6aPU2FALAaa4LWKQ8W1uA)

some domain llms: [Awesome-Domain-LLM](https://github.com/luban-agi/Awesome-Domain-LLM)

healcare models: [Awesome-Healthcare-Foundation-Models](https://github.com/Jianing-Qiu/Awesome-Healthcare-Foundation-Models)

# General Domain Instruction Models

| contributor                       | model/project                                                                                                                     | language | base model                                         | main feature                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| :-------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------- | -------- | :-------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Stanford                          | [Alpaca](https://github.com/tatsu-lab/stanford_alpaca)                                                                               | en       | LLaMA/OPT                                           | use 52K instruction-following data generated by Self-Instructt techniques to fine-tune 7B LLaMA,<br /> the resulting model,  Alpaca, behaves similarly to the `text-davinci-003` model on the Self-Instruct instruction-following evaluation suite.<br />Alpaca has inspired many follow-up models.                                                                                                                                                                                                                                                                                                                         |
| LianJiaTech                       | [BELLE](https://github.com/LianjiaTech/BELLE)                                                                                        | en/zh    | BLOOMZ-7B1-mt                                       | maybe the first Chinese model to follow Alpaca.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| THU                               | [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)                                                                                    | en/zh    | -                                                   | well-known Chinese model.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| Databricks                        | [Dolly](https://github.com/databrickslabs/dolly)                                                                                     | en       | GPT-J 6B                                            | use Alpaca data to fine-tune a 2-year-old model: GPT-J, which exhibits surprisingly high quality<br /> instruction following behavior not characteristic of the foundation model on which it is based.                                                                                                                                                                                                                                                                                                                                                                                                                            |
| @tloen                            | [Alpaca-LoRA](https://github.com/tloen/alpaca-lora)                                                                                  | en       | LLaMA-7B                                            | trained within hours on a single RTX 4090,<br />reproducing the [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) results using [low-rank adaptation (LoRA)](https://arxiv.org/pdf/2106.09685.pdf),<br />and can run on a Raspberry pi.                                                                                                                                                                                                                                                                                                                                                                                   |
| ColossalAI                        | [Coati7B]()                                                                                                                          | en/zh    | LLaMA-7B                                            | a large language model developed by the ColossalChat project                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| Shanghai AI Lab                   | [LLaMA-Adapter](https://github.com/ZrrSkywalker/LLaMA-Adapter)                                                                       | en       | LLaMA-7B                                            | Fine-tuning LLaMA to follow instructions within 1 Hour and 1.2M Parameters                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| AetherCortex                      | [Llama-X](https://github.com/AetherCortex/Llama-X)                                                                                   | en       | LLaMA                                               | Open Academic Research on Improving LLaMA to SOTA LLM.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| TogetherComputer                  | [OpenChatKit](https://github.com/togethercomputer/OpenChatKit)                                                                       | en       | GPT-NeoX-20B                                        | OpenChatKit provides a powerful, open-source base to create both specialized and general purpose chatbots for various applications.<br /> The kit includes an instruction-tuned language models, a moderation model, and an extensible retrieval system for including <br />up-to-date responses from custom repositories.                                                                                                                                                                                                                                                                                                         |
| nomic-ai                          | [GPT4All](https://github.com/nomic-ai/gpt4all)                                                                                       | en       | LLaMA                                               | trained on a massive collection of clean assistant data including code, stories and dialogue                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| @ymcui                            | [Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca)                                                                | en/zh    | LLaMA-7B/13B                                        | **expand the Chinese vocabulary** based on the original LLaMA and use Chinese data for secondary pre-training,<br /> further enhancing Chinese basic semantic understanding. Additionally, the project uses Chinese instruction data<br /> for fine-tuning on the basis of the Chinese LLaMA, significantly improving the model's understanding and execution of instructions.                                                                                                                                                                                                                                               |
| UC Berkley<br />Stanford<br />CMU | [Vicuna](https://github.com/lm-sys/FastChat)                                                                                         | en       | LLaMA-13B                                           | Impressing GPT-4 with 90% ChatGPT Quality.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| UCSD/SYSU                         | [baize](https://github.com/project-baize/baize)                                                                                      | en/zh    | LLaMA                                               | fine-tuned with[LoRA](https://github.com/microsoft/LoRA). It uses 100k dialogs generated by letting ChatGPT chat with itself. <br />Alpaca's data is also used to improve its performance.                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| UC Berkley                        | [Koala](https://github.com/young-geng/EasyLM)                                                                                        | en       | LLaMA                                               | Rather than maximizing*quantity* by scraping as much web data as possible, the team focus on collecting a small *high-quality* dataset.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| @imClumsyPanda                    | [langchain-ChatGLM](https://github.com/imClumsyPanda/langchain-ChatGLM)                                                              | en/zh    | ChatGLM-6B                                          | local knowledge based ChatGLM with langchain.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| @yangjianxin1                     | [Firefly](https://github.com/yangjianxin1/Firefly)                                                                                   | zh       | bloom-1b4-zh<br />bloom-2b6-zh                      | Instruction Tuning on Chinese dataset. Vocabulary pruning, ZeRO, and tensor parallelism<br /> are used to effectively reduce memory consumption and improve training efficiency.                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| microsoft                         | [GPT-4-LLM](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)                                                              | en/zh    | LLaMA                                               | aims to share data generated by GPT-4 for building an instruction-following LLMs with supervised learning and reinforcement learning.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| Hugging Face                      | [StackLLaMA](https://huggingface.co/trl-lib/llama-7b-se-rl-peft)                                                                     | en       | LLaMA                                               | trained on StackExchange data and the main goal is to serve as a tutorial and walkthrough on<br /> how to train model with RLHF and not primarily model performance.                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| Nebuly                            | [ChatLLaMA](https://github.com/nebuly-ai/nebullvm/tree/main/apps/accelerate/chatllam)                                                | en       | -                                                   | a library that allows you to create hyper-personalized ChatGPT-like assistants using your own data and the least amount of compute possible.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| @juncongmoo                       | [ChatLLaMA](https://github.com/juncongmoo/chatllama)                                                                                 | en       | LLaMA                                               | LLaMA-based RLHF model, runnable in a single GPU.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| @juncongmoo                       | [minichatgpt](https://github.com/juncongmoo/minichatgpt)                                                                             | en       | GPT/OPT ...                                         | To Train ChatGPT In 5 Minutes with ColossalAI.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| @LC1332                           | [Luotuo-Chinese-LLM](https://github.com/LC1332/Luotuo-Chinese-LLM)                                                                   | zh       | LLaMA/ChatGLM                                       | Instruction fine-tuned Chinese Language Models, with colab provided!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| @Facico                           | [Chinese-Vicuna](https://github.com/Facico/Chinese-Vicuna)                                                                           | zh       | LLaMA                                               | A Chinese Instruction-following LLaMA-based Model, fine-tuned with Lora, cpp inference supported, colab provided.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| @yanqiangmiffy                    | [InstructGLM](https://github.com/yanqiangmiffy/InstructGLM)                                                                          | en/zh    | ChatGLM-6B                                          | ChatGLM based instruction-following model, fine-tuned on a variety of data sources, supports deepspeed accelerating and LoRA.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| alibaba                           | [Wombat](https://github.com/GanjinZero/RRHF)                                                                                         | en       | LLaMA                                               | a novel learning paradigm called RRHF, as an alternative of RLHF,  is proposed, which scores responses generated by<br /> different sampling policies and learns to align them with human preferences through ranking loss. And the performance<br />is comparable to RLHF, with less models used in the process.                                                                                                                                                                                                                                                                                                              |
| @WuJunde                          | [alpaca-glassoff](https://github.com/WuJunde/alpaca-glassoff)                                                                        | en       | LLaMA                                               | a mini image-acceptable Chat AI can run on your own laptop,  based on[stanford-alpaca](https://github.com/tatsu-lab/stanford_alpaca) and [alpaca-lora](https://github.com/tloen/alpaca-lora).                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| @JosephusCheung                   | [Guanaco](https://huggingface.co/datasets/JosephusCheung/GuanacoDataset)                                                             | multi    | LLaMA-7B                                            | A Multilingual Instruction-Following Language Model.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| @FreedomIntelligence              | [LLM Zoo](https://github.com/FreedomIntelligence/LLMZoo)                                                                             | multi    | BLOOMZ/LLaMA                                        | a project that provides data, models, and evaluation benchmark for large language models.<br />model released: Phoenix, Chimera                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| SZU                               | [Linly](https://github.com/CVI-SZU/Linly)                                                                                            | en/zh    | LLaMA                                               | **expand the Chinese vocabulary**, full fine-tuned models, largest LLaMA-based Chinese models, aggregation of Chinese instruction data, reproduceable details..                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| @lamini-ai                        | [lamini](https://github.com/lamini-ai/lamini/)                                                                                       | multi    | -                                                   | data generator for generating instructions to train instruction-following LLMs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| Stability-AI                      | [StableVicuna](https://stability.ai/blog/stablevicuna-open-source-rlhf-chatbot)                                                      | en       | LLaMA                                               | a further instruction fine tuned and RLHF trained version of Vicuna v0 13b, with better performance than Vicuna.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| Hugging Face                      | [HuggingChat](https://huggingface.co/chat/)                                                                                          | en       | LLaMA                                               | seems to be the first one available to access as a platform that appears similar to ChatGPT.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| microsoft                         | [WizardLM](https://github.com/nlpxucan/WizardLM)                                                                                     | en       | LLaMA                                               | trained with 70k evolved instructions,[Evol-Instruct](https://github.com/nlpxucan/evol-instruct) is a novel method using LLMs instead of humans to automatically mass-produce<br /> open-domain instructions of various difficulty levels and skills range, to improve the performance of LLMs.                                                                                                                                                                                                                                                                                                                                      |
| FDU                               | [OpenChineseLLaMA](https://github.com/OpenLMLab/OpenChineseLLaMA)                                                                    | en/zh    | LLaMA-7B                                            | further pretrain LLaMA on Chinese data, improving LLaMA preformance on Chinese tasks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| @chenfeng357                      | [open-Chinese-ChatLLaMA](https://github.com/chenfeng357/open-Chinese-ChatLLaMA)                                                      | en/zh    | LLaMA                                               | The complete training code of the open-source Chinese-Llama model, including the full process from pre-training instructing and RLHF.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| @FSoft-AI4Code                    | [CodeCapybara](https://github.com/FSoft-AI4Code/CodeCapybara)                                                                        | en       | LLaMA                                               | Open Source LLaMA Model that Follow Instruction-Tuning for Code Generation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| @mbzuai-nlp                       | [LaMini-LM](https://github.com/mbzuai-nlp/LaMini-LM)                                                                                 | en       | LLaMA/Flan-T5 ...                                   | A Diverse Herd of Distilled Models from Large-Scale Instructions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| NTU                               | [Panda](https://github.com/dandelionsllm/pandallm)                                                                                   | en/zh    | LLaMA                                               | further pretraining on Chinese data, full-size of LLaMA models.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| IBM/CMU/MIT                       | [Dromedary](https://github.com/IBM/Dromedary)                                                                                        | en       | LLaMA-65B                                           | Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| @melodysdreamj                    | [WizardVicunaLM](https://github.com/melodysdreamj/WizardVicunaLM)                                                                    | multi    | Vicuna                                              | Wizard's dataset + ChatGPT's conversation extension + Vicuna's tuning method,<br /> achieving approximately 7% performance improvement over Vicuna.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| sambanovasystems                  | [BLOOMChat](https://huggingface.co/sambanovasystems/BLOOMChat-176B-v1)                                                               | multi    | BLOOM                                               | BLOOMChat is a 176 billion parameter multilingual chat model. It is instruction tuned from[BLOOM (176B)](https://huggingface.co/bigscience/bloom) on<br /> assistant-style conversation datasets and supports conversation, question answering and generative answers in multiple languages.                                                                                                                                                                                                                                                                                                                                          |
| [TII](https://www.tii.ae/)           | [Falcon-7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct)                                                               | en       | [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b)   | a 7B parameters causal decoder-only model built by[TII](https://www.tii.ae/) based on [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b) and finetuned on a mixture of chat/instruct datasets.                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| [TII](https://www.tii.ae/)           | [Falcon-40B-Instruct](https://huggingface.co/tiiuae/falcon-40b-instruct)                                                             | multi    | [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b) | a 40B parameters causal decoder-only model built by[TII](https://www.tii.ae/) based on [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b) and finetuned on a mixture of [Baize](https://github.com/project-baize/baize-chatbot).                                                                                                                                                                                                                                                                                                                                                                                                        |
| USTC, etc.                        | [ExpertLLaMA](https://github.com/OFA-Sys/ExpertLLaMA)                                                                                | en       | LLaMA                                               | use In-Context Learning to automatically write customized expert identity and find the quality quite satisfying.<br /> We then prepend corresponding expert identity to each instruction to produce augmented instruction-following data.<br /> We refer to the overall framework as **ExpertPrompting**, find more details in our [paper](https://arxiv.org/abs/2305.14688).                                                                                                                                                                                                                                                  |
| ZJU                               | [CaMA](https://github.com/zjunlp/CaMA)                                                                                               | en/zh    | LLaMA                                               | further pretrained on Chinese courpus without expansion of vocabulary; optimized on the Information Extraction (IE) tasks.<br />pre-training script is available, which includes transformations, construction, and loading of large-scale corpora, as well as the LoRA instruction fine-tuning script.                                                                                                                                                                                                                                                                                                                          |
| THU                               | [UltraChat](https://github.com/thunlp/UltraChat)                                                                                     | en       | LLaMA                                               | First, the UltraChat dataset provides a rich resource for the training of chatbots. Second, by fine-tuning the LLaMA model,<br /> the researchers successfully created a dialogue model UltraLLaMA with superior performance.                                                                                                                                                                                                                                                                                                                                                                                                      |
| RUC                               | [YuLan-Chat](https://github.com/RUC-GSAI/YuLan-Chat)                                                                                 | en/zh    | LLaMA                                               | developed based on fine-tuning LLaMA with high-quality English and Chinese instructions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| AI2                               | [Tülu](https://github.com/allenai/open-instruct)                                                                                    | en       | LLaMA/Pythia/OPT                                    | a suite of LLaMa models fully-finetuned on a strong mix of datasets.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| KAIST                             | [SelFee](https://github.com/kaistAI/SelFee)                                                                                          | en       | LLaMA                                               | Iterative Self-Revising LLM Empowered by Self-Feedback Generation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| @lyogavin                         | [Anima](https://github.com/lyogavin/Anima)                                                                                           | en/zh    | LLaMA                                               | trained based on QLoRA's[33B guanaco](https://huggingface.co/timdettmers/guanaco-33b), finetuned for 10000 steps.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| THU                               | [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B)                                                                                  | en/zh    | -                                                   | ChatGLM**2** -6B is the second-generation version of the open-source bilingual (Chinese-English) chat model [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B).<br /> It retains the smooth conversation flow and low deployment threshold of the first-generation model, while introducing the following new features:<br />- Stronger Performance<br />- Longer Context<br />- More Efficient Inference- More Open License                                                                                                                                                                                                  |
| OpenChat                          | [OpenChat](https://github.com/imoneoi/openchat)                                                                                      | en       | LLaMA, etc.                                         | a series of open-source language models fine-tuned on a small, yet diverse and high-quality dataset of multi-round conversations.<br /> Specifically, we utilize only ~6K GPT-4 conversations directly filtered from the ~90K ShareGPT conversations.<br /> Despite the small size of the dataset, OpenLLMs has demonstrated remarkable performance.                                                                                                                                                                                                                                                                               |
| CAS                               | [BayLing](https://github.com/ictnlp/BayLing)                                                                                         | multi    | LLaMA                                               | BayLing is an English/Chinese LLM equipped with advanced language alignment,<br /> showing superior capability in English/Chinese generation, instruction following and multi-turn interaction.                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| stabilityai                       | [FreeWilly](https://huggingface.co/stabilityai/FreeWilly1-Delta-SafeTensor)/[FreeWilly2](https://huggingface.co/stabilityai/FreeWilly2) | en       | LLaMA/LLaMA2                                        | `FreeWilly` is a Llama65B model fine-tuned on an [Orca](https://arxiv.org/pdf/2306.02707.pdf) style Dataset.<br />`FreeWilly2` is a Llama2 70B model finetuned on an [Orca](https://arxiv.org/pdf/2306.02707.pdf) style Dataset.<br />`FreeWilly2` outperforms Llama2 70B on the [huggingface Open LLM leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard).                                                                                                                                                                                                                                             |
| alibaba                           | [Qwen-7B](https://github.com/QwenLM/Qwen-7B)                                                                                         | en/zh    | -                                                   | 7B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| ZJU                               | [KnowLM](https://github.com/zjunlp/KnowLM)                                                                                           | en/zh    | LLaMA                                               | With the rapid development of deep learning technology, large language models such as ChatGPT have made substantial strides in the realm of natural language processing.<br /> However, these expansive models still encounter several challenges in acquiring and comprehending knowledge, including the difficulty of updating knowledge and potential knowledge<br /> discrepancies and biases, collectively known as**knowledge fallacies** .<br />The KnowLM project endeavors to tackle these issues by launching an open-source large-scale knowledgable language model framework and releasing corresponding models. |
| NEU                               | [TechGPT](https://github.com/neukg/TechGPT)                                                                                          | en/zh    | LLAMA                                               | TechGPT mainly strengthens the following three types of tasks:<br />- Various information extraction tasks such as relation triplet extraction with "knowledge graph construction" as the core<br />- Various intelligent question-and-answer tasks centered on "reading comprehension".<br />- Various sequence generation tasks such as keyword generation with "text understanding" as the core.                                                                                                                                                                                                                               |
| @MiuLab                           | [Taiwan-LLaMa](https://github.com/MiuLab/Taiwan-LLaMa)                                                                               | en/zh    | LLaMA2                                              | Traditional Chinese LLMs for Taiwan.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| Xwin-LM                           | [Xwin-LM](https://github.com/Xwin-LM/Xwin-LM)                                                                                        | en       | LLaMA2                                              | Xwin-LM aims to develop and open-source alignment technologies for large language models, including supervised fine-tuning (SFT),<br /> reward models (RM), reject sampling, reinforcement learning from human feedback (RLHF), etc. Our first release, built-upon on the<br /> Llama2 base models, ranked**TOP-1** on [AlpacaEval](https://tatsu-lab.github.io/alpaca_eval/). Notably, it's **the first to surpass GPT-4** on this benchmark.                                                                                                                                                                            |
| wenge-research                    | [YaYi](https://github.com/wenge-research/YaYi)                                                                                       | en/zh    | LLaMA/LLaMA2                                        | [YaYi](https://www.wenge.com/yayi/index.html) was fine-tuned on millions of artificially constructed high-quality domain data. This training data covers five key domains:<br /> media publicity, public opinion analysis, public safety, financial risk control, and urban governance, encompassing over a hundred natural language instruction tasks.                                                                                                                                                                                                                                                                               |

# Alternatives To Transformer

(maybe successors?)

| contributor | method                                     | main feature                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| ----------- | ------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| BlinkDL     | [RWKV-LM](https://github.com/BlinkDL/RWKV-LM) | RWKV is an RNN with transformer-level LLM performance. It can be directly trained like a GPT (parallelizable).<br /> So it's combining the best of RNN and transformer - great performance, fast inference, saves VRAM, fast training, "infinite" ctx_len, and free sentence embedding.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| msra        | [RetNet](https://arxiv.org/abs/2307.08621)    | simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention.<br /> Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent.<br /> Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost**O**(**1**) inference, which improves decoding throughput,<br /> latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity,<br /> where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results,<br /> parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. |
| stanford    | [Bapcpack](https://backpackmodels.science)    | A[Backpack](https://arxiv.org/abs/2305.16765) is a drop-in replacement for a Transformer that provides new tools for **interpretability-through-control** while still enabling strong language models.<br /> Backpacks decompose the predictive meaning of words into components non-contextually, and aggregate them by a weighted sum, allowing for precise, predictable interventions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |

# Multi-Modal

| contributor                               | project                                            | language | base model                              | main feature                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| ----------------------------------------- | -------------------------------------------------- | -------- | --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| BaihaiAIen/zh                             | [IDPChat](https://github.com/BaihaiAI/IDPChat)        | en/zh    | LLaMA-13B<br />Stable Diffusion         | Open Chinese multi-modal model, single GPU runnable, easy to deploy, UI provided.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| KAUST                                     | [MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4) | en/zh    | LLaMA                                   | MiniGPT-4 aligns a frozen visual encoder from BLIP-2 with a frozen LLM, Vicuna, using just one projection layer,<br />and yields many emerging vision-language capabilities similar to those demonstrated in GPT-4.                                                                                                                                                                                                                                                                                                                                                                          |
| UW–Madison/MSR<br />/Columbia University | [LLaVA](https://github.com/haotian-liu/LLaVA)         | en       | LLaMA                                   | visual instruction tuning is proposed, towards building large language and vision models with GPT-4 level capabilities.                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| NUS/THU                                   | [VPGTrans](https://github.com/VPGTrans/VPGTrans)      | en       | LLaMA/OPT/<br />Flan-T5/BLIP-2<br />... | transferring VPG across LLMs to build VL-LLMs at significantly lower cost. The GPU hours<br /> can be reduced over 10 times and the training data can be reduced to around 10%.<br />Two novel VL-LLMs are released via VPGTrans, including **[VL-LLaMA](https://github.com/VPGTrans/VPGTrans#vl-llama)** and  **[VL-Vicuna](https://github.com/VPGTrans/VPGTrans#vl-vicuna)**.<br />**VL-LLaMA** is a multimodal version LLaMA by transferring the BLIP-2 OPT-6.7B to LLaMA via VPGTrans.<br />**VL-Vicuna** is a GPT-4-like multimodal chatbot, based on the Vicuna LLM. |
| CAS, etc                                  | [X-LLM](https://github.com/phellonchen/X-LLM)         | en/zh    | ChatGLM-6B                              | X-LLM converts multi-modalities (images, speech, videos) into foreign languages using X2L interfaces and feed them into<br /> a large Language Model (ChatGLM) to accomplish a Multimodal LLM, achieving impressive multimodal chat capabilities.                                                                                                                                                                                                                                                                                                                                             |
| NTU                                       | [Otter](https://github.com/Luodian/Otter)             | en       | OpenFlamingo                            | a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo),<br /> trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning.<br />Futhermore, optimize OpenFlamingo's implementation, democratizing the required<br /> training resources from 1x A100 GPU to 4x RTX-3090 GPUs.                                                                                                                                                                                                                                          |
| XMU                                       | [LaVIN](https://github.com/luogen1996/LaVIN)          | en       | LLaMA                                   | propose a novel and affordable solution for vision-language instruction tuning, namely Mixture-of-Modality Adaptation (MMA).<br /> Particularly, MMA is an end-to-end optimization regime, which connects the image encoder and LLM via lightweight adapters.<br /> Meanwhile, we also propose a novel routing algorithm in MMA, which can help the model automatically shifts the reasoning paths<br /> for single- and multi-modal instructions.                                                                                                                                            |

see also: [awesome-Multimodal-Large-Language-Models](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)

# Data

## Pretrain Data

| contributor      | data/project                                                      | language | main feature                                               |
| ---------------- | ----------------------------------------------------------------- | -------- | ---------------------------------------------------------- |
| TogetherComputer | [RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data) | en       | An Open Source Recipe to Reproduce LLaMA training dataset. |
| @goldsmith       | [Wikipedia](https://github.com/goldsmith/Wikipedia)                  | multi    | A Pythonic wrapper for the Wikipedia API.                  |

## Instruction Data

see [Alpaca-CoT data collection](https://github.com/PhoebusSi/Alpaca-CoT/blob/main/CN_README.md#3-%E6%95%B0%E6%8D%AE%E9%9B%86%E5%90%88-data-collection)

| contributor | data                                                    | language | main feature                                                                                                                  |
| ----------- | ------------------------------------------------------- | -------- | ----------------------------------------------------------------------------------------------------------------------------- |
| salesforce  | [DialogStudio](https://github.com/salesforce/DialogStudio) | en       | DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection and Instruction-Aware Models for Conversational AI. |

## Synthetic Data Generation

| contributor | method                                                                                              | main feature                                                                                                                                                                                                                                                                                                               |
| ----------- | --------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| UW, etc.    | [self-instruct](https://github.com/yizhongw/self-instruct)                                             | using the model's own generations to create a large collection of instructional data.                                                                                                                                                                                                                                      |
| @LiuHC0428  | [Reliable-Self-Instruction](https://github.com/LiuHC0428/LAW-GPT#%E7%9F%A5%E8%AF%86%E9%97%AE%E7%AD%94) | use ChatGPT to generate some questions and answers based on a given text.                                                                                                                                                                                                                                                  |
| PKU         | [Evol-Instruct](https://github.com/nlpxucan/evol-instruct)                                             | a novel method, proposed in[WizardLM](https://github.com/nlpxucan/WizardLM),  by using LLMs instead of humans to automatically mass-produce open-domain<br /> instructions of various difficulty levels and skills range, to improve the performance of LLMs.                                                               |
| KAUST, etc. | [CAMEL](https://github.com/lightaime/camel)                                                            | a novel communicative agent framework named*role-playing* is proposed, which involves using *inception prompting* to guide chat agents<br /> toward task completion while maintaining consistency with human intentions.<br />*role-playing* can be used to generate conversational data in a specific task/domain. |
| @chatarena  | [ChatArena](https://github.com/chatarena/chatarena)                                                    | a library that provides multi-agent language game environments and facilitates research about autonomous LLM agents and their social interactions.<br />it provides a flexible framework to define multiple players, environments and the interactions between them, based on Markov Decision Process.                     |

# Evaluation

| contributor      | method                                              | main feature                                                                                                                                                             |
| ---------------- | --------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| -                | human evaluation                                    | -                                                                                                                                                                        |
| OpenAI           | GPT-4/ChatGPT                                       | -                                                                                                                                                                        |
| PKU/CMU/MSRA ... | [PandaLM](https://github.com/WeOpenML/PandaLM)         | Reproducible and Automated Language Model Assessment.                                                                                                                    |
| UCB              | [Chatbot Arena](https://github.com/lm-sys/FastChat)    | Chat with two anonymous models side-by-side and vote for which one is better,<br /> then use the Elo rating system to calculate the relative performance of the models. |
| Stanford         | [AlpacaEval](https://github.com/tatsu-lab/alpaca_eval) | GPT-4/Claude evaluation on[AlpacaFarm](https://github.com/tatsu-lab/alpaca_farm/tree/main) dataset.                                                                        |
| clueai           | [SuperCLUElyb](https://www.superclueai.com/)           | Chinese version of[Chatbot Arena](https://github.com/lm-sys/FastChat) developed by clueai.                                                                                 |
| SJTU, etc.       | [Auto-J](https://github.com/GAIR-NLP/auto-j)           | a new open-source generative judge that can effectively evaluate different LLMs on how they align to human preference.                                                   |

## Benchmark

| contributor | benchmark                                                        | main feature                                                                                                                                                                                                                                   |
| ----------- | ---------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| princeton   | [SWE-bench](https://github.com/princeton-nlp/SWE-bench)             | a benchmark for evaluating large language models on real world software issues collected from GitHub. Given a*codebase* and an  *issue*,<br /> a language model is tasked with generating a *patch* that resolves the described problem. |
| microsoft   | [AGIEval](https://github.com/ruixiangcui/AGIEval)                   | a human-centric benchmark specifically designed to evaluate the general abilities of foundation models in tasks pertinent to human cognition and problem-solving.                                                                              |
| clueai      | [SuperCLUE-Agent](https://github.com/CLUEbenchmark/SuperCLUE-Agent) | Agent evaluation benchmark based on Chinese native tasks.                                                                                                                                                                                      |

## LeaderBoard

opencompass, huggingface

# Framework/ToolKit/Platform

| contributor | project                                                                                    | main feature                                                                                                                                                                                                                                                                                                                       |
| ----------- | ------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| CAS         | [Alpaca-CoT](https://github.com/PhoebusSi/Alpaca-CoT)                                         | extend CoT data to Alpaca to boost its reasoning ability.<br />aims at building an instruction finetuning (IFT) platform with extensive instruction collection (especially the CoT datasets)<br />and a unified interface for various large language models.                                                                       |
| @hiyouga    | [ChatGLM-Efficient-Tuning](https://github.com/hiyouga/ChatGLM-Efficient-Tuning)               | efficient fine-tuning ChatGLM-6B with PEFT.                                                                                                                                                                                                                                                                                        |
| @hiyouga    | [LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning)                   | Fine-tuning LLaMA with PEFT (PT+SFT+RLHF with QLoRA).                                                                                                                                                                                                                                                                              |
| @jianzhnie  | [Efficient-Tuning-LLMs](https://github.com/jianzhnie/Efficient-Tuning-LLMs)                   | Efficient Finetuning of QLoRA LLMs.                                                                                                                                                                                                                                                                                                |
| ColossalAI  | [ColossalChat](https://github.com/hpcaitech/ColossalAI/blob/main/applications/Chat/README.md) | An open-source low cost solution for cloning[ChatGPT](https://openai.com/blog/chatgpt/) with a complete RLHF pipeline.                                                                                                                                                                                                                |
| microsoft   | [deepspeed-chat](https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-chat)     | Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales.                                                                                                                                                                                                                                                      |
| LAION-AI    | [Open Assistant](https://github.com/LAION-AI/Open-Assistant)                                  | a project meant to give everyone access to a great chat based large language model.                                                                                                                                                                                                                                                |
| HKUST       | [LMFlow](https://github.com/OptimalScale/LMFlow)                                              | an extensible, convenient, and efficient toolbox for finetuning large machine learning models,<br /> designed to be user-friendly, speedy and reliable, and accessible to the entire community.                                                                                                                                    |
| UCB         | [EasyLM](https://github.com/young-geng/EasyLM)                                                | EasyLM is a one stop solution for pre-training, finetuning, evaluating and serving LLMs in JAX/Flax.<br /> EasyLM can scale up LLM training to hundreds of TPU/GPU accelerators by leveraging JAX's pjit functionality.                                                                                                            |
| @CogStack   | [OpenGPT](https://github.com/CogStack/opengpt)                                                | A framework for creating grounded instruction based datasets and training conversational domain expert Large Language Models (LLMs).                                                                                                                                                                                               |
| HugAILab    | [HugNLP](https://github.com/HugAILab/HugNLP)                                                  | a unified and comprehensive NLP library based on HuggingFace Transformer.                                                                                                                                                                                                                                                          |
| ProjectD-AI | [LLaMA-Megatron-DeepSpeed](https://github.com/ProjectD-AI/LLaMA-Megatron-DeepSpeed)           | Ongoing research training transformer language models at scale, including: BERT & GPT-2.                                                                                                                                                                                                                                           |
| @PanQiWei   | [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ)                                              | An easy-to-use LLMs quantization package with user-friendly apis, based on GPTQ algorithm.                                                                                                                                                                                                                                         |
| alibaba     | [swift](https://github.com/modelscope/swift)                                                  | SWIFT (Scalable lightWeight Infrastructure for Fine-Tuning) is an extensible framwork designed to faciliate lightweight model fine-tuning and inference.<br /> It integrates implementations for various efficient fine-tuning methods, by embracing approaches that is parameter-efficient, memory-efficient, and time-efficient. |
| alibaba     | [Megatron-LLaMA](https://github.com/alibaba/Megatron-LLaMA)                                   | to facilitate the training of LLaMA-based models and reduce the cost on occupying hardware resources,<br /> Alibaba decides to release the internal optimized Megatron-LLaMA training framework to the community.                                                                                                                  |

# Alignment

| contributor | method                                                                                   | used in                                                   | main feature                                                                                                                                                                                                                                                                                                         |
| ----------- | ---------------------------------------------------------------------------------------- | --------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| -           | [IFT](https://arxiv.org/pdf/2109.01652.pdf)                                                 | [ChatGPT](https://openai.com/blog/chatgpt/)                  | Instruction Fine-Tuning.                                                                                                                                                                                                                                                                                            |
| -           | [RLHF](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback)            | [ChatGPT](https://openai.com/blog/chatgpt/)                  | RL from Human Feedback.                                                                                                                                                                                                                                                                                             |
| Anthropic   | [RLAIF](https://arxiv.org/abs/2212.08073)                                                   | [Claude](https://www.anthropic.com/index/introducing-claude) | RL from AI Feedback.                                                                                                                                                                                                                                                                                                 |
| alibaba     | [RRHF](https://arxiv.org/pdf/2304.05302v1.pdf)                                              | [Wombat](https://github.com/GanjinZero/RRHF)                 | a novel learning paradigm called RRHF, as an alternative of RLHF,  is proposed, which scores responses generated by<br />different sampling policies and learns to align them with human preferences through ranking loss. And the performance<br />is comparable to RLHF, with less models used in the process. |
| HKUST       | [RAFT](https://optimalscale.github.io/LMFlow/examples/raft.html)                            | -                                                         | RAFT is a new alignment algorithm, which is more efficient than conventional (PPO-based) RLHF.                                                                                                                                                                                                                      |
| IBM/CMU/MIT | [SELF-ALIGN](https://arxiv.org/abs/2305.03047)                                              | [Dromedary](https://github.com/IBM/Dromedary)                | combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision.                                                                                                                                                                             |
| PKU         | [CVA](https://github.com/PKU-Alignment/safe-rlhf#constrained-value-alignment-via-safe-rlhf) | [Beaver](https://github.com/PKU-Alignment/safe-rlhf)         | Constrained Value Alignment via Safe RLHF.                                                                                                                                                                                                                                                                           |
| tencent     | [RLTF](https://github.com/Zyq-scut/RLTF)                                                    | -                                                         | Reinforcement Learning from Unit Test Feedback.                                                                                                                                                                                                                                                                      |

# Multi-Language

## vocabulary expansion

according to the official [FAQ](https://github.com/facebookresearch/llama/blob/main/FAQ.md#4-other-languages) in LLaMA repo, there's not many tokens other than latin languages, so one of the efforts is to expand the vocabulary, some works are shown below:

| contributor    | model/project                                                      | language | base model      | main feature                                                                                                                                                                                                                                                                                                                                                                               |
| -------------- | ------------------------------------------------------------------ | -------- | ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| @ymcui         | [Chinese-LLaMA-Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) | zh       | LLaMA            |                                                                                                                                                                                                                                                                                                                                                                                            |
| SZU            | [Linly](https://github.com/CVI-SZU/Linly)                             | en/zh    | LLaMA            | full-size LLaMA, further pretrained on Chineses Corpus.                                                                                                                                                                                                                                                                                                                                    |
| @Neutralzz     | [BiLLa](https://github.com/Neutralzz/BiLLa)                           | en/zh    | LLaMA-7B         | further pretrained on[Wudao](https://www.sciencedirect.com/science/article/pii/S2666651021000152)、[PILE](https://arxiv.org/abs/2101.00027)、[WMT](https://www.statmt.org/wmt22/translation-task.html).                                                                                                                                                                                             |
| @pengxiao-song | [LaWGPT](https://github.com/pengxiao-song/LaWGPT)                     | zh       | LLaMA/ChatGLM    | expand the vocab with Chinese legal terminologies, instruction fine-tuned on data generated using self-instruct.                                                                                                                                                                                                                                                                          |
| IDEA           | [Ziya](https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-Pretrain-v1)   | en/zh    | LLaMA            | large-scale pre-trained model based on LLaMA with 13 billion parameters.<br />We optimizes LLaMAtokenizer on chinese, and incrementally train 110 billion tokens of data based on LLaMa-13B model,<br />which significantly improved the understanding and generation ability on Chinese.                                                                                                  |
| OpenBuddy      | [OpenBuddy](https://github.com/OpenBuddy/OpenBuddy)                   | multi    | LLaMA/Falcon ... | Built upon Tii's Falcon model and Facebook's LLaMA model, OpenBuddy is fine-tuned to include an extended vocabulary,<br /> additional common characters, and enhanced token embeddings. By leveraging these improvements and multi-turn dialogue datasets,<br /> OpenBuddy offers a robust model capable of answering questions and performing translation tasks across various languages. |
| FDU            | [CuteGPT](https://github.com/Abbey4799/CuteGPT)                       | en/zh    | LLaMA            | CuteGPT expands the Chinese vocabulary and performs pre-training on the Llama model, improving its ability to understand Chinese.<br /> Subsequently, it is fine-tuned with conversational instructions to enhance the model's ability to understand instructions.                                                                                                                         |
| FlagAlpha      | [FlagAlpha](https://github.com/FlagAlpha/Llama2-Chinese)              | en/zh    | LLaMA/LLaMA2     | based on largs-scale Chinese data, and starting from pre-training, the Chinese abilities of the models are being continuously and iteratively upgraded.                                                                                                                                                                                                                                    |

# Efficient Training/Fine-Tuning

| contributor                              | method                                                        | main feature                                                                                                                                                                                                                                                                                                                                        |
| ---------------------------------------- | ------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| microsoft                                | [LoRA](https://arxiv.org/abs/2106.09685)                         | Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices<br /> into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks.                                                                                        |
| stanford                                 | [Prefix Tuning](https://aclanthology.org/2021.acl-long.353/)     | a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen<br /> and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix.                                                                                                                  |
| THU                                      | [P-Tuning](https://arxiv.org/abs/2103.10385)                     | P-tuning leverages few continuous free parameters to serve as prompts fed as the input to the pre-trained language models.<br />We then optimize the continuous prompts using gradient descent as an alternative to discrete prompt searching.                                                                                                    |
| THU/BAAI/<br />Shanghai Qi Zhi Institute | [P-Tuning v2](https://arxiv.org/pdf/2110.07602.pdf)              | a novel empirical finding that properly optimized prompt tuning can be comparable to fine-tuning universally across various model scales and NLU tasks.<br />Technically, P-tuning v2 is not conceptually novel. It can be viewed as an optimized and adapted implementation of Deep Prompt Tuning.                                             |
| Google                                   | [Prompt Tuning](https://arxiv.org/abs/2104.08691)                | a simple yet effective mechanism for learning "soft prompts" to condition frozen language models to perform specific downstream tasks.<br />Prompt Tuning can be seen as a simplification of "prefix tuning".                                                                                                                                     |
| GT/Princeton/microsoft                   | [AdaLoRA](https://arxiv.org/abs/2303.10512)                      | adaptively allocates the parameter budget among weight matrices according to their importance score.<br /> In particular, AdaLoRA parameterizes the incremental updates in the form of singular value decomposition.                                                                                                                                |
| UW                                       | [QLoRA](https://github.com/artidoro/qlora)                       | an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving<br /> full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA).                                  |
| FDU                                      | [LOMO](https://github.com/OpenLMLab/LOMO)                        | a new optimizer,**LO**w-Memory **O**ptimization ( **LOMO** ), which fuses the gradient computation and the parameter update in one step to reduce memory usage,<br />which enables the full parameter fine-tuning of a 7B model on a single RTX 3090, or a 65B model on a single machine with 8×RTX 3090, each with 24GB memory. |
| MBZUAI, Transmute AI Lab<br />Meta, CMU  | [GLoRA](https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA) | Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations,<br /> providing more flexibility and capability across diverse tasks and datasets.                                                                                                       |
| UMass Lowell                             | [ReLoRA](https://github.com/Guitaricet/relora)                   | ReLoRA performs a high-rank update and achieves performance similar to regular neural network training.<br /> The components of ReLoRA include initial full-rank training of the neural network, LoRA training, restarts, a jagged learning rate schedule, and partial optimizer resets.                                                        |
| Huawei                                   | [QA-LoRA](https://arxiv.org/pdf/2309.14717.pdf)                  | equips the original LoRA with two-fold abilities:<br />(i) during fine-tuning, the LLM’s weights are quantized (e.g., into INT4) to reduce time and memory usage;<br /> (ii) after fine-tuning, the LLM and auxiliary weights are naturally integrated into a quantized model without loss of accuracy.                                         |
| UMD, etc.                                | [NEFTune](https://github.com/neelsjain/NEFTune/tree/main)        | we propose to add random noise to the embedding vectors of the training data during the forward pass of fine-tuning. We show that this simple trick<br /> can improve the outcome of instruction fine-tuning, often by a large margin, with no additional compute or data overhead.                                                                 |

# Low-Cost Inference

## quantization

| contributor | algorithm                                                                           | main feature                                                                                                                                                                                                                                                                                              |
| ----------- | ----------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| UW, etc.    | [SpQR](https://github.com/Vahe1994/SpQR)                                               | a new compressed format and quantization technique which enables for the first time near-lossless compression of LLMs across model scales,<br /> while reaching similar compression levels to previous methods.                                                                                          |
| THU         | [Train_Transformers_with_INT4](https://github.com/xijiu9/Train_Transformers_with_INT4) | For forward propagation, we identify the challenge of outliers and propose a Hadamard quantizer to suppress the outliers.<br /> For backpropagation, we leverage the structural sparsity of gradients by proposing bit splitting and leverage score sampling techniques to quantize gradients accurately. |
| INTEL       | [neural-compressor](https://github.com/intel/neural-compressor)                        | targeting to provide unified APIs for network compression technologies, such as low precision quantization, sparsity, pruning, knowledge distillation,<br /> across different deep learning frameworks to pursue optimal inference performance.                                                           |

## projects

| contributor                      | project                                                                                           | main feature                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| -------------------------------- | ------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| @ggerganov                       | [llama.cpp](https://github.com/ggerganov/llama.cpp)                                                  | c/cpp implementation for llama and some other models, using quantization.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| @NouamaneTazi                    | [bloomz.cpp](https://github.com/NouamaneTazi/bloomz.cpp)                                             | C++ implementation for BLOOM inference.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| @mlc-ai                          | [MLC LLM](https://github.com/mlc-ai/mlc-llm)                                                         | a universal solution that allows any language models to be deployed natively on a diverse set of hardware backends and native applications,<br />plus a productive framework for everyone to further optimize model performance for their own use cases.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| alibaba                          | [ChatGLM-MNN](https://github.com/wangzhaode/ChatGLM-MNN)                                             | converts the ChatGLM-6B model to MNN and performs inference using C++.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| Jittor                           | [JittorLLMs](https://github.com/Jittor/JittorLLMs)                                                   | Significantly reduce hardware costs (by 80%), currently known as the lowest-cost deployment library, supports multiple platforms.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| OpenBMB                          | [BMInf](https://github.com/OpenBMB/BMInf)                                                            | BMInf supports running models with more than 10 billion parameters on a single NVIDIA GTX 1060 GPU in its minimum requirements.<br /> In cases where the GPU memory supports the large model inference (such as V100 or A100),<br /> BMInf still has a significant performance improvement over the existing PyTorch implementation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| hpcaitech                        | [EnergonAI](https://github.com/hpcaitech/EnergonAI)                                                  | With tensor parallel operations, pipeline parallel wrapper, distributed checkpoint loading, and customized CUDA kernel,<br /> EnergonAI can enable efficient parallel inference for larges-scale models.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| MegEngine                        | [InferLLM](https://github.com/MegEngine/InferLLM)                                                    | a lightweight LLM model inference framework that mainly references and borrows from[the llama.cpp project](https://github.com/ggerganov/llama.cpp).<br /> llama.cpp puts almost all core code and kernels in a single file and use a large number of macros, making it difficult for developers to read and modify.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| @saharNooby                      | [rwkv.cpp](https://github.com/saharNooby/rwkv.cpp)                                                   | a port of[BlinkDL/RWKV-LM](https://github.com/BlinkDL/RWKV-LM) to [ggerganov/ggml](https://github.com/ggerganov/ggml).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| FMInference                      | [FlexGen](https://github.com/FMInference/FlexGen)                                                    | FlexGen is a high-throughput generation engine for running large language models with limited GPU memory.<br /> FlexGen allows**high-throughput** generation by IO-efficient offloading, compression, and  **large effective batch sizes** .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| huggingface<br />bigcode-project | [starcoder.cpp](https://github.com/bigcode-project/starcoder.cpp)                                    | C++ implemention for 💫 StarCoder inference using the[ggml](https://github.com/ggerganov/ggml) library.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| CMU                              | [SpecInfer](https://github.com/flexflow/FlexFlow/tree/inference)                                     | SpecInfer is an open-source distributed multi-GPU system that accelerates generative LLM inference with**speculative inference** and  **token tree verification**.<br /> A key insight behind SpecInfer is to combine various collectively boost-tuned small speculative models (SSMs) to jointly predict the LLM’s outputs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |
| @ztxz16                          | [fastllm](https://github.com/ztxz16/fastllm)                                                         | full-platform pure c++ llm acceleration library, supports moss, chatglm, baichuan models,  runs smoothly on mobile phones.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| UCB                              | [vllm](https://github.com/vllm-project/vllm)                                                         | a fast and easy-to-use library for LLM inference and serving. fast with Efficient management of attention key and value memory with**PagedAttention.**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| stanford                         | [mpt-30B-inference](https://github.com/abacaj/mpt-30B-inference)                                     | Run inference on the latest MPT-30B model using your CPU. This inference code uses a[ggml](https://github.com/ggerganov/ggml) quantized model.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| Shanghai AI Lab                  | [lmdeploy](https://github.com/InternLM/lmdeploy)                                                     | a toolkit for compressing, deploying, and serving LLM.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| @turboderp                       | [ExLlama](https://github.com/turboderp/exllama) / [ExLlamaV2](https://github.com/turboderp/exllamav2) | A fast inference library for running LLMs locally on modern consumer-class GPUs                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| PyTorch                          | [ExecuTorch](https://github.com/pytorch/executorch)                                                  | End-to-end solution for enabling on-device AI across mobile and edge devices for PyTorch models.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| Xorbitsai                        | [Xinference](https://github.com/xorbitsai/inference)                                                 | a powerful and versatile library designed to serve language, speech recognition, and multimodal models.<br /> With Xorbits Inference, you can effortlessly deploy and serve your or state-of-the-art built-in models using just a single command.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| NVIDIA                           | [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)                                               | TensorRT-LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and build[TensorRT](https://developer.nvidia.com/tensorrt) engines that contain<br /> state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT-LLM also contains components to create Python and C++ runtimes<br /> that execute those TensorRT engines. It also includes a [backend](https://github.com/triton-inference-server/tensorrtllm_backend) for integration with the [NVIDIA Triton Inference Server](https://developer.nvidia.com/nvidia-triton-inference-server); a production-quality system to serve LLMs.<br /> Models built with TensorRT-LLM can be executed on a wide range of configurations going from a single GPU to<br /> multiple nodes with multiple GPUs (using [Tensor Parallelism](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/nemo_megatron/parallelisms.html#tensor-parallelism) and/or [Pipeline Parallelism](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/nemo_megatron/parallelisms.html#pipeline-parallelism)). |
| @sabetAI                         | [Batched LoRAs](https://github.com/sabetAI/BLoRA)                                                    | Maximize GPU util by routing inference through multiple LoRAs in the same batch.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| microsoft                        | [LLMLingua](https://github.com/microsoft/LLMLingua)                                                  | LLMLingua, that uses a well-trained small language model after alignment, such as GPT2-small or LLaMA-7B, to detect the unimportant tokens in the prompt<br /> and enable inference with the compressed prompt in black-box LLMs, achieving up to 20x compression with minimal performance loss.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |

# Prompting

[Prompt Engineering Guide](https://www.promptingguide.ai/)

| contributor     | method                                                                                                    | main feature                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| --------------- | --------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Google          | [CoT](https://arxiv.org/pdf/2201.11903.pdf)                                                                  | a technique that allows large language models (LLMs) to solve a problem as a series of intermediate steps before giving a final answer.                                                                                                                                                                                                                                                                                                                                                                                               |
| Princeton, etc. | ToT([Yao et el. (2023)](https://arxiv.org/abs/2305.10601) and [Long (2023)](https://arxiv.org/abs/2305.08291)) | ToT maintains a tree of thoughts, where thoughts represent coherent language sequences that serve as intermediate steps toward solving a problem.<br /> This approach enables an LM to self-evaluate the progress intermediate thoughts make towards solving a problem through a deliberate reasoning process.                                                                                                                                                                                                                        |
| SJTU, etc.      | [GoT](https://arxiv.org/pdf/2305.16582.pdf)                                                                  | we propose Graph-of-Thought (GoT) reasoning, which models human thought processes not only as a chain but also as a graph. By representing thought units as nodes<br />and connections between them as edges, our approach captures the non-sequential nature of human thinking and allows for a more realistic modeling of thought processes.                                                                                                                                                                                      |
| Princeton, etc. | [ReAct](https://github.com/ysymyth/ReAct)                                                                    | LLMs are used to generate both*reasoning traces* and *task-specific actions* in an interleaved manner.                                                                                                                                                                                                                                                                                                                                                                                                                            |
| SJTU            | [Meta-CoT](https://github.com/Anni-Zou/Meta-CoT)                                                             | **Meta-CoT** is a generalizable CoT prompting method in mixed-task scenarios where the type of input questions is unknown. It consists of three phases:<br /> (i)  *scenario identification* : categorizes the scenario of the input question;<br /> (ii)  *demonstration selection* : fetches the ICL demonstrations for the categorized scenario;<br /> (iii)  *answer derivation* : performs the answer inference by feeding the LLM with the prompt comprising the fetched ICL demonstrations and the input question. |

# Safety

| contributor | method                                                    | main feature                                                            |
| ----------- | --------------------------------------------------------- | ----------------------------------------------------------------------- |
| thu-coai    | [Safety-Prompts](https://github.com/thu-coai/Safety-Prompts) | Chinese safety prompts for evaluating and improving the safety of LLMs. |

# Truthfulness

| contributor | method                                        | main feature                                                                                                                                                                                                                                                                                                                                               |
| ----------- | --------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Harvard     | [ITI](https://github.com/likenneth/honest_llama) | ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads.<br /> This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark.<br /> On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5 to 65.1. |

# Exceeding Context Window

## Extending Context Window

| contributor      | method                                                                                                | main feature                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| ---------------- | ----------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| UW, etc.         | [ALiBi](https://github.com/ofirpress/attention_with_linear_biases)                                       | Instead of adding position embeddings at the bottom of the transformer stack,<br /> ALiBi adds a linear bias to each attention score, allowing the model to be trained on,<br /> for example, 1024 tokens, and then do inference on 2048 (or much more) tokens without any finetuning.                                                                                                                                                                   |
| DeepPavlov, etc. | [RMT](https://arxiv.org/abs/2304.11062)                                                                  | use a recurrent memory to extend the context length.                                                                                                                                                                                                                                                                                                                                                                                                      |
| bytedance        | [SCM](https://arxiv.org/abs/2304.11062)                                                                  | unleash infinite-length input capacity for large-scale language models.                                                                                                                                                                                                                                                                                                                                                                                    |
| Meta             | [Position Interpolation](https://arxiv.org/pdf/2306.15595.pdf)                                           | extends the context window sizes of RoPE-based  pretrained LLMs such as LLaMA models to up to 32768 with minimal fine-tuning (within 1000 steps).<br />Position Interpolation linearly down-scales the input position indices to match the original context window size, rather than extrapolating beyond<br /> the trained context length which may lead to catastrophically high attention scores that completely ruin the self-attention mechanism. |
| UCB              | [LongChat](https://github.com/DachengLi1/LongChat)                                                       | Instead of forcing the LLaMA model to adapt to position_ids > 2048, we condense position_ids > 2048 to be within 0 to 2048 (the same machenism as[Position Interpolation](https://arxiv.org/pdf/2306.15595.pdf), surprisingly!).<br />we observed that our LongChat-13B-16K model reliably retrieves the first topic, with comparable accuracy to gpt-3.5-turbo.                                                                                              |
| microsoft        | [LongNet](https://github.com/microsoft/unilm/tree/master#revolutionizing-transformers-for-mllms-and-agi) | replaces the attention of vanilla Transformers with a novel component named**dilated attention**, and successfully scale the sequence length to 1 billion tokens.                                                                                                                                                                                                                                                                                   |
| IDEAS NCBR, etc. | [LongLLaMA](https://github.com/CStanKonrad/long_llama)                                                   | LongLLaMA is built upon the foundation of[OpenLLaMA](https://github.com/openlm-research/open_llama) and fine-tuned using the [Focused Transformer (FoT)](https://arxiv.org/abs/2307.03170) method, and is capable of handling long contexts of 256k tokens or even more.                                                                                                                                                                                         |
| Abacus.AI        | [Giraffe](https://huggingface.co/abacusai/Giraffe-v2-13b-32k)                                            | a range of experiments with different schemes for extending context length capabilities of Llama are conducted.                                                                                                                                                                                                                                                                                                                                            |
| TogetherComputer | [Llama-2-7B-32K-Instruct](https://huggingface.co/togethercomputer/Llama-2-7B-32K-Instruct)               | long-context chat model finetuned from[Llama-2-7B-32K](https://huggingface.co/togethercomputer/Llama-2-7B-32K), over high-quality instruction and chat data.                                                                                                                                                                                                                                                                                                  |
| Jianlin Su       | [ReRoPE](https://github.com/bojone/rerope)                                                               | set a window with size$w$, the interval between positions inside the window is **1**, while the interval outside the window is $\frac 1 k$.                                                                                                                                                                                                                                                                                                      |
| CUHK/MIT         | [longlora](https://github.com/dvlab-research/longlora)                                                   | an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost.                                                                                                                                                                                                                                                                                                               |

## Without Extending Context Window

| contributor  | method                                                     | main feature                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| ------------ | ---------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| MIT/Meta/CMU | [StreamingLLM](https://github.com/mit-han-lab/streaming-llm)  | deploy LLMs for**infinite-length inputs** without sacrificing efficiency and performance.<br />an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence length without any fine-tuning.<br /> We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.<br /> In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment.<br /> In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. |
| UCB          | [Ring Attention](https://browse.arxiv.org/pdf/2310.01889.pdf) | We present a distinct approach, Ring Attention, which leverages blockwise computation of self-attention to distribute long sequences across multiple devices while overlapping the communication of<br /> key-value blocks with the computation of blockwise attention. Ring Attention enables training and inference of sequences that are up to device count times longer than those of prior memory-efficient Transformers,<br /> effectively eliminating the memory constraints imposed by individual devices. Extensive experiments on language modeling tasks demonstrate the effectiveness of Ring Attention in allowing large sequence input size<br /> and improving performance.                        |
| UCB          | [MemGPT](https://github.com/cpacker/MemGPT)                   | a system that intelligently manages different memory tiers in LLMs in order to effectively provide extended context within the LLM's limited context window.<br /> For example, MemGPT knows when to push critical information to a vector database and when to retrieve it later in the chat, enabling perpetual conversations.                                                                                                                                                                                                                                                                                                                                                                                      |
| FDU, etc.    | [ScalingRoPE](https://github.com/OpenLMLab/scaling-rope)      | we first observe that fine-tuning a RoPE-based LLM with either a smaller or larger base in pre-training context length could significantly enhance its extrapolation performance.<br /> After that, we propose Scaling Laws of RoPE-based Extrapolation, a unified framework from the periodic perspective,<br /> to describe the relationship between the extrapolation performance and base value as well as tuning context length.                                                                                                                                                                                                                                                                              |

# Knowledge Editing

Must-read Papers on Model Editing: [ModelEditingPapers](https://github.com/zjunlp/ModelEditingPapers)

| contributor | method                         | main feature                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| ----------- | ------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| MIT, etc.   | [ROME](https://rome.baulab.info/) | First, we trace the causal effects of hidden state activations within GPT using causal mediation analysis to identify the specific modules that mediate recall of a fact about a subject.<br /> Our analysis reveals that feedforward MLPs at a range of middle layers are decisive when processing the last token of the subject name.<br />Second, we test this finding in model weights by introducing a Rank-One Model Editing method (ROME) to alter the parameters that determine a feedfoward layer’s behavior at the decisive token.<br />Despite the simplicity of the intervention, we find that ROME is similarly effective to other modelediting approaches on a standard zero-shot relation extraction benchmark. |

## Implementations

| contributor | project                                      | main feature                                                                                                                                                                                                                                                                                                                                                                               |
| ----------- | -------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| PKU         | [FastEdit](https://github.com/hiyouga/FastEdit) | injecting**fresh** and **customized** knowledge into large language models efficiently using one single command.                                                                                                                                                                                                                                                               |
| ZJU         | [EasyEdit](https://github.com/zjunlp/EasyEdit)  | a Python package for edit Large Language Models (LLM) like `GPT-J`, `Llama`, `GPT-NEO`, `GPT2`, `T5`(support models from **1B** to  **65B** ), <br />the objective of which is to alter the behavior of LLMs efficiently within a specific domain without negatively impacting performance across other inputs. It is designed to be easy to use and easy to extend. |

# External Knowledge

allowing the model to access external knowledge, such as KG、databases.

| contributor    | project                                                                     | main feature                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| -------------- | --------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| @jerryjliu     | [LlamaIndex](https://github.com/jerryjliu/llama_index)                         | provides a central interface to connect your LLM's with external data.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |
| @imClumsyPanda | [langchain-ChatGLM](https://github.com/imClumsyPanda/langchain-ChatGLM)        | local knowledge based ChatGLM with[langchain](https://github.com/hwchase17/langchain).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
| @wenda-LLM     | [wenda](https://github.com/wenda-LLM/wenda)                                    | an LLM calling platform[ ](https://github.com/wenda-LLM/wenda)designed to find and design automatic execution actions for small model plug-in<br /> knowledge bases to achieve the same generation ability as large models.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| @csunny        | [DB-GPT](https://github.com/csunny/DB-GPT)                                     | build a complete private large model solution for all database-based scenarios.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| THU, BAAI, ZJU | [ChatDB](https://github.com/huchenxucs/ChatDB)                                 | a novel framework integrating symbolic memory with LLMs. ChatDB explores ways of augmenting LLMs with symbolic memory to handle contexts of arbitrary lengths.<br /> Such a symbolic memory framework is instantiated as an LLM with a set of SQL databases. The LLM generates SQL instructions to manipulate the SQL databases<br /> autonomously (including insertion, selection, update, and deletion), aiming to complete a complex task requiring multi-hop reasoning and long-term symbolic memory.                                                                                                                                                                                                                                                                                                                                                                                                                          |
| IDEA           | [Ziya-Reader](https://modelscope.cn/models/Fengshenbang/Ziya-Reader-13B-v1.0/) | "Ziya-Reader-13B-v1.0" is a knowledge question-answering model. It can accurately answer questions given questions and knowledge documents,<br /> and is suitable for both multi-document and single-document question-answering. The model has an 8k context window, and compared to models with longer windows,<br /> we have achieved victory in evaluations across multiple long-text tasks. The tasks include multi-document question-answering, synthetic tasks (document retrieval), and long-text summarization.<br />Additionally, the model also demonstrates excellent generalization capabilities, enabling it to be used for general question-answering.<br /> Its performance on our general ability evaluation set surpassed that of Ziya-Llama-13B.                                                                                                                                                                |
| docker         | [GenAI Stack](https://github.com/docker/genai-stack)                           | significantly simplify the entire process by integrating Docker with the Neo4j graph database, LangChain model linking technology, and Ollama for running Large Language models (LLM)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| ARC53          | [DocsGPT](https://github.com/arc53/DocsGPT)                                    | GPT-powered chat for documentation, chat with your documents.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |
| UW, etc.       | [Self-RAG](https://github.com/AkariAsai/self-rag)                              | Unlike a widely-adopted Retrieval-Augmented Generation (RAG) approach,**Self-RAG** retrieves on demand (e.g., can retrieve multiple times or completely skip retrieval) given diverse queries,<br /> and criticize its own generation from multiple fine-grained aspects by predicting **reflection tokens** as an integral part of generation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| RUC            | [StructGPT](https://github.com/JBoRu/StructGPT)                                | Inspired by the studies on tool augmentation for LLMs, we develop an Iterative Reading-thenReasoning (IRR) framework to solve question answering tasks based on structured data, called StructGPT.<br /> In this framework, we construct the specialized interfaces to collect relevant evidence from structured data (i.e., reading), and let LLMs concentrate on the reasoning task based on the collected information (i.e., reasoning).<br /> Specially, we propose an invokinglinearization-generation procedure to support LLMs in reasoning on the structured data with the help of the interfaces. By iterating this procedure with provided interfaces,<br /> our approach can gradually approach the target answers to a given query. Experiments conducted on three types of structured data show that StructGPT greatly improves the performance of LLMs,<br /> under the few-shot and zero-shot settings. |

# External Tools

## Using Existing Tools

allowing the model to access external tools, such as search engine、api.

| contributor   | project                                          | base model | main feature                                                                                                                                                                                                                                                                                                                                         |
| ------------- | ------------------------------------------------ | ---------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| UCB/microsoft | [Gorilla](https://github.com/ShishirPatil/gorilla/) | LLaMA      | invokes 1,600+ (and growing) API calls accurately while reducing hallucination.                                                                                                                                                                                                                                                                      |
| THU           | [ToolLLaMA](https://github.com/OpenBMB/ToolBench)   | LLaMA      | This project aims to construct**open-source, large-scale, high-quality** instruction tuning SFT data to facilitate the construction<br /> of powerful LLMs with general **tool-use** capability. We provide the dataset, the corresponding training and evaluation scripts,<br /> and a capable model ToolLLaMA fine-tuned on ToolBench. |

## Make New Tools

| contributor  | project                                       | main feature                                              |
| ------------ | --------------------------------------------- | --------------------------------------------------------- |
| Google, etc. | [LATM](https://github.com/ctlllll/LLM-ToolMaker) | LLMs create their own reusable tools for problem-solving. |

# Agent

| contributor           | project                                                                 | driven by | main feature                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| --------------------- | ----------------------------------------------------------------------- | --------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| @Significant-Gravitas | [Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT)               | GPT-4     | chains together LLM "thoughts", to autonomously achieve whatever goal you set.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| @yoheinakajima        | [BabyAGI](https://github.com/yoheinakajima/babyagi)                        | GPT       | The main idea behind this system is that it creates tasks based on the result of previous tasks and a predefined objective.<br />The script then uses OpenAI's natural language processing (NLP) capabilities to create new tasks based on the objective,<br />and Chroma/Weaviate to store and retrieve task results for context.                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
| microsoft             | [HuggingGPT](https://github.com/microsoft/JARVIS)                          | GPT-4     | Language serves as an interface for LLMs to connect numerous AI models for solving complicated AI tasks!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| microsoft/NCSU        | [ReWOO](https://github.com/billxbf/ReWOO)                                  | -         | detaches the reasoning process from external observations, thus significantly reducing token consumption.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |
| Stanford              | [generative_agents](https://github.com/joonspk-research/generative_agents) | GPT       | Generative Agents: Interactive Simulacra of Human Behavior.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| THU, etc.             | [AgentVerse](https://github.com/OpenBMB/AgentVerse)                        | GPT       | 🤖 AgentVerse 🪐 provides a flexible framework that simplifies the process of building custom multi-agent environments for large language models (LLMs).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |
| BUAA, etc.            | [TrafficGPT](https://github.com/lijlansg/TrafficGPT)                       | GPT       | By seamlessly intertwining large language model and traffic expertise, TrafficGPT not only advances traffic<br />management but also offers a novel approach to leveraging AI capabilities in this domain.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |
| microsoft, etc.       | [ToRA](https://github.com/microsoft/ToRA)                                  | -         | ToRA is a series of Tool-integrated Reasoning LLM Agents designed to solve challenging mathematical reasoning problems by interacting with tools.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |
| HKU                   | [OpenAgents](https://github.com/xlang-ai/OpenAgents)                       | -         | an open platform for using and hosting language agents in the wild of everyday life.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
| THU                   | [XAgent](https://github.com/OpenBMB/XAgent)                                | -         | an open-source experimental Large Language Model (LLM) driven autonomous agent that can automatically solve various tasks.<br /> It is designed to be a general-purpose agent that can be applied to a wide range of tasks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| Nvidia, etc.          | [Eureka](https://github.com/eureka-research/Eureka)                        | -         | a**human-level** reward design algorithm powered by LLMs. Eureka exploits the remarkable zero-shot generation, code-writing, and in-context improvement<br /> capabilities of state-of-the-art LLMs, such as GPT-4, to perform in-context evolutionary optimization over reward code. The resulting rewards can then be used to<br /> acquire complex skills via reinforcement learning. Eureka generates reward functions that outperform expert human-engineered rewards without any task-specific<br /> prompting or pre-defined reward templates. In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies,<br /> Eureka outperforms human expert on **83%** of the tasks leading to an average normalized improvement of  **52%** . |
| THU                   | [AgentTuning](https://github.com/THUDM/AgentTuning)                        | -         | **AgentTuning** represents the very first attempt to instruction-tune LLMs using interaction trajectories across multiple agent tasks.<br /> Evaluation results indicate that AgentTuning enables the agent capabilities of LLMs with robust generalization on unseen agent tasks while remaining good on general language abilities.                                                                                                                                                                                                                                                                                                                                                                                                                                                        |
| microsoft             | [AutoGen](https://github.com/microsoft/autogen)                            | -         | AutoGen is a framework that enables the development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are<br /> customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| PKU                   | [RestGPT](https://github.com/Yifan-Song793/RestGPT)                        | -         | we connect LLMs with**RESTful APIs** and tackle the practical challenges of planning, API calling, and response parsing. To fully evaluate the performance of RestGPT, we propose  **RestBench**,<br /> a high-quality benchmark which consists of two real-world scenarios and human-annotated instructions with gold solution paths.<br />RestGPT adopts an iterative coarse-to-fine online planning framework and uses an executor to call RESTful APIs.                                                                                                                                                                                                                                                                                                                            |

paper list: [LLM-Agent-Paper-List](https://github.com/WooooDyy/LLM-Agent-Paper-List)

# LLMs as XXX

| contributor     | method                                    | main feature                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
| --------------- | ----------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Google DeepMind | [OPRO](https://arxiv.org/pdf/2309.03409.pdf) | large language models as optimizers.Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs)<br />as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that<br />contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. |

# Similar Collections

| collections of open instruction-following llms                                                             |
| ---------------------------------------------------------------------------------------------------------- |
| [开源微调大型语言模型（LLM）合集](https://zhuanlan.zhihu.com/p/628716889)                                     |
| [机器之心SOTA!模型](https://sota.jiqizhixin.com/models/list)                                                  |
| [Awesome Totally Open Chatgpt](https://github.com/nichtdax/awesome-totally-open-chatgpt)                      |
| [LLM-Zoo](https://github.com/DAMO-NLP-SG/LLM-Zoo)                                                             |
| [Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM)                                                     |
| [🤗 Open LLM Leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)                   |
| [Open LLMs](https://github.com/eugeneyan/open-llms)                                                           |
| [Awesome-Chinese-LLM](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM)                                      |
| [Awesome Pretrained Chinese NLP Models](https://github.com/lonePatient/awesome-pretrained-chinese-nlp-models) |
| [LLMSurvey](https://github.com/RUCAIBox/LLMSurvey)                                                            |
